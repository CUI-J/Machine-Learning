{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation for Linear Regression\n",
    "\n",
    "Regularisation is a technique commonly used in Machine Learning to prevent overfitting. It consists on adding terms to the objective function such that the optimisation procedure avoids solutions that just learn the training data. Popular techniques for regularisation in Supervised Learning include Lasso Regression, Ridge Regression and the Elastic Net. \n",
    "\n",
    "In this Assignment, you will be looking at Ridge Regression and devising equations to optimise the objective function in Ridge Regression using two methods: a closed-form derivation and the update rules for stochastic gradient descent. You will then use those update rules for making predictions on a Air Quaility dataset.\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Let us start with a data set for training $\\mathcal{D} = \\{\\mathbf{y}, \\mathbf{X}\\}$, where the vector $\\mathbf{y}=[y_1, \\cdots, y_n]^{\\top}$ and $\\mathbf{X}$ is the design matrix from Lab 3, this is, \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} = \n",
    "                \\begin{bmatrix}\n",
    "                        1 & x_{1,1} & \\cdots & x_{1, D}\\\\\n",
    "                        1 & x_{2,1} & \\cdots & x_{2, D}\\\\\n",
    "                   \\vdots &  \\vdots\\\\\n",
    "                        1 & x_{n,1} & \\cdots & x_{n, D}\n",
    "                \\end{bmatrix}\n",
    "               = \n",
    "               \\begin{bmatrix}\n",
    "                      \\mathbf{x}_1^{\\top}\\\\\n",
    "                       \\mathbf{x}_2^{\\top}\\\\\n",
    "                          \\vdots\\\\\n",
    "                        \\mathbf{x}_n^{\\top}\n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Our predictive model is going to be a linear model\n",
    "\n",
    "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "\n",
    "where $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$.\n",
    "\n",
    "The **objetive function** we are going to use has the following form\n",
    "\n",
    "$$ J(\\mathbf{w}, \\alpha) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))^2 + \\frac{\\alpha}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "where $\\alpha>0$ is known as the *regularisation* parameter.\n",
    "\n",
    "The first term on the right-hand side (rhs) of the expression for $J(\\mathbf{w}, \\alpha)$ is very similar to the least-squares objective function we have seen before, for example in Lab 3. The only difference is on the term $\\frac{1}{n}$ that we use to normalise the objective with respect to the number of observations in the dataset. \n",
    "\n",
    "The first term on the rhs is what we call the \"fitting\" term whereas the second term in the expression is the regularisation term. Given $\\alpha$, the two terms in the expression have different purposes. The first term is looking for a value of $\\mathbf{w}$ that leads the squared-errors to zero. While doing this, $\\mathbf{w}$ can take any value and lead to a solution that it is only good for the training data but perhaps not for the test data. The second term is regularising the behavior of the first term by driving the $\\mathbf{w}$ towards zero. By doing this, it restricts the possible set of values that $\\mathbf{w}$ might take according to the first term. The value that we use for $\\alpha$ will allow a compromise between a value of $\\mathbf{w}$ that exactly fits the data (first term) or a value of $\\mathbf{w}$ that does not grow too much (second term).\n",
    "\n",
    "This type of regularisation has different names: ridge regression, Tikhonov regularisation or $\\ell_2$ norm regularisation. \n",
    "\n",
    "### Question 1: $J(\\mathbf{w}, \\alpha)$ in matrix form (2 marks)\n",
    "\n",
    "Write the expression for $J(\\mathbf{w}, \\alpha)$ in matrix form. Include ALL the steps necessary to reach the expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "The **objetive function** we are going to use has the following form\n",
    "\n",
    "$$ J(\\mathbf{w}, \\alpha) = \\frac{1}{n}\\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))^2 + \\frac{\\alpha}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "We also known that\n",
    "\n",
    "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "$$ \\sum_{j=0}^D w_j^2 = \\mathbf{w}^{\\top}\\mathbf{w} $$\n",
    "\n",
    "We write our objective in the folowing form\n",
    "$$\n",
    "J(\\mathbf{w}, \\alpha) = \\frac{1}{n}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\frac{\\alpha}{2}\\mathbf{w}^{\\top}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "where the vector $\\mathbf{y}=[y_1, \\cdots, y_n]^{\\top}$,  $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$ and \n",
    "\\begin{align*}\n",
    "    \\mathbf{X} = \n",
    "                \\begin{bmatrix}\n",
    "                        1 & x_{1,1} & \\cdots & x_{1, D}\\\\\n",
    "                        1 & x_{2,1} & \\cdots & x_{2, D}\\\\\n",
    "                   \\vdots &  \\vdots\\\\\n",
    "                        1 & x_{n,1} & \\cdots & x_{n, D}\n",
    "                \\end{bmatrix}\n",
    "               = \n",
    "               \\begin{bmatrix}\n",
    "                      \\mathbf{x}_1^{\\top}\\\\\n",
    "                       \\mathbf{x}_2^{\\top}\\\\\n",
    "                          \\vdots\\\\\n",
    "                        \\mathbf{x}_n^{\\top}\n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "So we can write **objetive function** in this matrix form\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, \\alpha) = \\frac{1}{n}( \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\n",
    "\\end{bmatrix} - \\begin{bmatrix}\\mathbf{x}_1^{\\top}\\mathbf{w}\\\\\\mathbf{x}_2^{\\top}\\mathbf{w}\\\\\\vdots\\\\\\mathbf{x}_n^{\\top}\\mathbf{w}\\end{bmatrix})^\\top( \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\n",
    "\\end{bmatrix} - \\begin{bmatrix}\\mathbf{x}_1^{\\top}\\mathbf{w}\\\\\\mathbf{x}_2^{\\top}\\mathbf{w}\\\\\\vdots\\\\\\mathbf{x}_n^{\\top}\\mathbf{w}\\end{bmatrix}) + \\frac{\\alpha}{2}[w_0\\; w_1\\; \\cdots \\; w_D]\\begin{bmatrix}[w_0\\\\w_1\\\\\\vdots\\\\w_n]\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, \\alpha) = \\frac{1}{n}[(y_1 - \\mathbf{x}_1^{\\top}\\mathbf{w})^2 + (y_2 - \\mathbf{x}_2^{\\top}\\mathbf{w})^2 + \\cdots + (y_n - \\mathbf{x}_n^{\\top}\\mathbf{w})^2] + \\frac{\\alpha}{2}(w_0^2 + w_1^2 + \\cdots + w_n^2)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the objective function with respect to $\\mathbf{w}$\n",
    "\n",
    "There are two ways we can optimise the objective function with respect to $\\mathbf{w}$. The first one leads to a closed form expression for $\\mathbf{w}$ and the second one using an iterative optimisation procedure that updates the value of $\\mathbf{w}$ at each iteration by using the gradient of the objective function with respect to $\\mathbf{w}$,\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d J(\\mathbf{w}, \\alpha)}{d\\mathbf{w}},\n",
    "$$\n",
    "where $\\eta$ is the *learning rate* parameter and $\\frac{d J(\\mathbf{w}, \\alpha)}{d\\mathbf{w}}$ is the gradient of the objective function.\n",
    "\n",
    "### Question 2: Derivative of $J(\\mathbf{w}, \\alpha)$ wrt $\\mathbf{w}$ (2 marks)\n",
    "\n",
    "Find the closed-form expression for $\\mathbf{w}$ by taking the derivative of $J(\\mathbf{w}, \\alpha)$ with respect to \n",
    "$\\mathbf{w}$, equating to zero and solving for $\\mathbf{w}$. Write the expression in matrix form. \n",
    "\n",
    "Also, write down the specific update rule for $\\mathbf{w}_{\\text{new}}$ by using the equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 Answer\n",
    "\n",
    "Write your answer to the question in this box.\n",
    "\n",
    "We have the following expression\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, \\alpha) = \\frac{1}{n}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\frac{\\alpha}{2}\\mathbf{w}^{\\top}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "we'll expand the brackets in the quadratic form to obtain a series of scalar terms.\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, \\alpha) = \\frac{1}{n}\\mathbf{y}^\\top\\mathbf{y} - \\frac{2}{n}\\mathbf{y}^\\top\\mathbf{X}\\mathbf{w} + \\frac{1}{n}\\mathbf{w}^\\top\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} + \\frac{\\alpha}{2}\\mathbf{w}^{\\top}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "Then we take the derivative of $J(\\mathbf{w}, \\alpha)$ with respect to $\\mathbf{w}$\n",
    "\n",
    "$$\n",
    "\\frac{d J(\\mathbf{w}, \\alpha)}{d\\mathbf{w}} = - \\frac{2}{n}\\mathbf{X}^\\top\\mathbf{y} + \\frac{2}{n}\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} + \\alpha\\mathbf{I}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{I}$ is an identity matrix.\n",
    "\n",
    "$$\n",
    "\\mathbf{0} = - \\frac{2}{n}\\mathbf{X}^\\top\\mathbf{y} + \\frac{2}{n}\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} + \\alpha\\mathbf{I}\\mathbf{w}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{0}$ is a vector of zeros. Rearranging this equation we find the solution to be \n",
    "\n",
    "$$\n",
    "\\mathbf{w} = [\\frac{2}{n}\\mathbf{X}^\\top\\mathbf{X} + \\alpha\\mathbf{I}]^{-1}\\frac{2}{n}\\mathbf{X}^\\top\\mathbf{y}\n",
    "$$\n",
    "\n",
    "We know that $\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d J(\\mathbf{w}, \\alpha)}{d\\mathbf{w}}$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta(- \\frac{2}{n}\\mathbf{X}^\\top\\mathbf{y} + \\frac{2}{n}\\mathbf{X}^\\top\\mathbf{X}\\mathbf{w} + \\alpha\\mathbf{I}\\mathbf{w})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ridge regression to predict air quality\n",
    "\n",
    "Our dataset comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). We are going to use ridge regression for predicting air quality. The description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Air+Quality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip -> .\\AirQualityUCI.zip\n",
      "[===========================   ]   1.328/1.472MB                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [==============================]   1.472/1.472MB                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "import pods\n",
    "pods.util.download_url('https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip')\n",
    "import zipfile\n",
    "zip = zipfile.ZipFile('./AirQualityUCI.zip', 'r')\n",
    "for name in zip.namelist():\n",
    "    zip.extract(name, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .csv version of the file has some typing issues, so we use the excel version\n",
    "import pandas as pd \n",
    "air_quality = pd.read_excel('./AirQualityUCI.xlsx', usecols=range(2,15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the rows in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>2.7</td>\n",
       "      <td>1172.00</td>\n",
       "      <td>324</td>\n",
       "      <td>11.781323</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>206.0</td>\n",
       "      <td>796.50</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1641.00</td>\n",
       "      <td>1186.25</td>\n",
       "      <td>12.775</td>\n",
       "      <td>51.974999</td>\n",
       "      <td>0.764340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8218</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1052.00</td>\n",
       "      <td>-200</td>\n",
       "      <td>4.538932</td>\n",
       "      <td>739.75</td>\n",
       "      <td>236.6</td>\n",
       "      <td>768.00</td>\n",
       "      <td>142.1</td>\n",
       "      <td>912.75</td>\n",
       "      <td>1201.50</td>\n",
       "      <td>3.250</td>\n",
       "      <td>52.175000</td>\n",
       "      <td>0.407446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3.4</td>\n",
       "      <td>1541.00</td>\n",
       "      <td>218</td>\n",
       "      <td>16.174457</td>\n",
       "      <td>1185.00</td>\n",
       "      <td>263.0</td>\n",
       "      <td>770.00</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1888.75</td>\n",
       "      <td>1406.50</td>\n",
       "      <td>11.675</td>\n",
       "      <td>63.650001</td>\n",
       "      <td>0.871937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7427</th>\n",
       "      <td>1.6</td>\n",
       "      <td>1103.75</td>\n",
       "      <td>-200</td>\n",
       "      <td>6.872632</td>\n",
       "      <td>851.25</td>\n",
       "      <td>366.7</td>\n",
       "      <td>687.00</td>\n",
       "      <td>98.4</td>\n",
       "      <td>1100.50</td>\n",
       "      <td>1084.75</td>\n",
       "      <td>4.325</td>\n",
       "      <td>82.900000</td>\n",
       "      <td>0.696836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>825.50</td>\n",
       "      <td>-200</td>\n",
       "      <td>3.772916</td>\n",
       "      <td>698.00</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1016.75</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1331.00</td>\n",
       "      <td>541.50</td>\n",
       "      <td>25.675</td>\n",
       "      <td>36.900000</td>\n",
       "      <td>1.199091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CO(GT)  PT08.S1(CO)  NMHC(GT)   C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
       "807      2.7      1172.00       324  11.781323        1042.00    206.0   \n",
       "8218     1.1      1052.00      -200   4.538932         739.75    236.6   \n",
       "134      3.4      1541.00       218  16.174457        1185.00    263.0   \n",
       "7427     1.6      1103.75      -200   6.872632         851.25    366.7   \n",
       "3925  -200.0       825.50      -200   3.772916         698.00   -200.0   \n",
       "\n",
       "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)       T         RH  \\\n",
       "807         796.50    120.0       1641.00      1186.25  12.775  51.974999   \n",
       "8218        768.00    142.1        912.75      1201.50   3.250  52.175000   \n",
       "134         770.00     97.0       1888.75      1406.50  11.675  63.650001   \n",
       "7427        687.00     98.4       1100.50      1084.75   4.325  82.900000   \n",
       "3925       1016.75   -200.0       1331.00       541.50  25.675  36.900000   \n",
       "\n",
       "            AH  \n",
       "807   0.764340  \n",
       "8218  0.407446  \n",
       "134   0.871937  \n",
       "7427  0.696836  \n",
       "3925  1.199091  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable corresponds to the CO(GT) variable of the first column. The following columns correspond to the variables in the feature vectors, *e.g.*, PT08.S1(CO) is $x_1$ up until AH which is $x_D$. The original dataset also has a date and a time columns that we are not going to use in this assignment.\n",
    "\n",
    "Before designing our predictive model, we need to think about three stages: the preprocessing stage, the training stage and the validation stage. The three stages are interconnected and *it is important to remember that the testing data that we use for validation has to be set aside before preprocessing*. Any preprocessing that you do has to be done only on the training data and several key statistics need to be saved for the test stage.\n",
    "\n",
    "Separating the dataset into training and test before any preprocessing has happened help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
    "\n",
    "We are going to use *hold-out validation* for testing our predictive model so we need to separate the dataset into a training set and a test set.\n",
    "\n",
    "### Question 3: Splitting the dataset (1 mark)\n",
    "\n",
    "Split the dataset into a training set and a test set. The training set should have 70% of the total observations and the test set, the 30%. For making the random selection make sure that you use a random seed that corresponds to the last five digits of your student UCard. Make sure that you comment your code.\n",
    "\n",
    "#### Question 3 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CO(GT)  PT08.S1(CO)  NMHC(GT)   C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
      "7076  -200.0   854.750000      -200   4.266063     725.250000   -200.0   \n",
      "3453     0.7   868.250000      -200   3.937417     707.250000     63.0   \n",
      "3387  -200.0  1105.750000      -200  13.765874    1109.000000    129.0   \n",
      "5043  -200.0  1335.500000      -200  16.785655    1203.500000   -200.0   \n",
      "8244     0.5   893.000000      -200   1.076022     506.500000     59.4   \n",
      "9075     3.9  1445.000000      -200  18.825234    1263.250000    550.5   \n",
      "2499     1.8   958.750000      -200   9.254615     949.250000    104.0   \n",
      "1180     1.4  1145.750000       156   8.587110     923.000000     79.0   \n",
      "580   -200.0  1469.250000      -200  14.336906    1127.500000   -200.0   \n",
      "6900     1.2   874.250000      -200   2.548045     622.750000    125.0   \n",
      "1063     2.6  1348.500000       251  13.400779    1097.000000    148.0   \n",
      "8121     2.5  1185.250000      -200   5.902703     807.250000   -200.0   \n",
      "2539     1.1   974.750000      -200   5.706840     798.000000     51.0   \n",
      "8686     1.4   906.750000      -200   4.789679     752.750000    313.2   \n",
      "3174  -200.0  1127.500000      -200  10.009991     978.000000     92.0   \n",
      "676      1.3  1020.250000        70   4.293956     726.750000     81.0   \n",
      "4474  -200.0   875.250000      -200   4.041180     713.000000   -200.0   \n",
      "6773     1.4  1060.750000      -200   5.738410     799.500000    200.0   \n",
      "1612     1.4  1033.750000      -200   5.860116     805.250000     92.0   \n",
      "3003     2.6  1029.250000      -200  13.628473    1104.500000    203.0   \n",
      "4811     0.9   893.500000      -200   5.628240     794.250000    179.0   \n",
      "8531     0.2   730.750000      -200   0.274336     407.750000     32.2   \n",
      "781      0.5   865.000000        36   0.945425     493.500000     39.0   \n",
      "1449  -200.0   802.750000      -200   1.344278     531.250000   -200.0   \n",
      "2626     0.4   837.750000      -200   2.691331     632.250000     24.0   \n",
      "8746     1.0   987.500000      -200   3.347581     673.250000    164.7   \n",
      "3814     1.0   871.000000      -200   4.956727     761.250000     61.0   \n",
      "848   -200.0   899.250000      -200   2.097105     591.250000   -200.0   \n",
      "5602  -200.0   931.333333      -200   5.422630     784.333333    173.0   \n",
      "614   -200.0  1630.500000      -200  25.790426    1449.000000   -200.0   \n",
      "...      ...          ...       ...        ...            ...      ...   \n",
      "3186  -200.0  1115.750000      -200  12.381849    1062.750000    140.0   \n",
      "7852     2.1  1109.000000      -200   7.148008     863.250000    359.7   \n",
      "8633     1.1   977.750000      -200   5.171480     772.000000    161.6   \n",
      "5294  -200.0   875.250000      -200   4.804320     753.500000   -200.0   \n",
      "5633     2.4  1239.750000      -200  10.977346    1013.500000    303.0   \n",
      "8855     3.4  1324.000000      -200  14.360264    1128.250000    492.3   \n",
      "4233  -200.0   868.000000      -200   3.076155     656.750000   -200.0   \n",
      "2401     3.5  1215.750000      -200  20.780977    1318.000000    218.0   \n",
      "4388     2.5  1159.750000      -200  16.182652    1185.250000    396.0   \n",
      "6633     2.0   973.750000      -200   6.446089     832.250000   -200.0   \n",
      "8553     0.5   787.750000      -200   0.875741     486.250000   -200.0   \n",
      "5444     2.3  1211.250000      -200  12.513760    1067.250000    230.0   \n",
      "692      2.8  1144.000000       294  13.903863    1113.500000    181.0   \n",
      "8193     0.4   821.250000      -200   1.104567     509.250000   -200.0   \n",
      "1419  -200.0  1072.000000      -200   9.949965     975.750000   -200.0   \n",
      "1867  -200.0  1072.500000      -200  12.076378    1052.250000   -200.0   \n",
      "2344     2.4  1125.750000      -200  12.243257    1058.000000    168.0   \n",
      "6910     4.3  1305.000000      -200  17.942598    1237.750000    587.0   \n",
      "3369  -200.0   786.750000      -200   2.364627     610.250000   -200.0   \n",
      "2356     2.5  1066.750000      -200  13.674208    1106.000000    143.0   \n",
      "5908     1.2   903.500000      -200   4.809205     753.750000    193.0   \n",
      "8394     1.2   992.250000      -200   3.693973     693.500000    265.8   \n",
      "1353     0.4   742.500000      -200   1.030001     502.000000   -200.0   \n",
      "7393     5.7  1516.750000      -200  28.351566    1511.750000    728.4   \n",
      "8538     0.7   782.500000      -200   1.941878     579.750000    174.1   \n",
      "5016  -200.0  1699.750000      -200  40.514244    1781.500000   -200.0   \n",
      "3455     0.9   912.250000      -200   5.257451     776.250000     61.0   \n",
      "784      1.0  1040.250000        55   2.900667     645.750000     71.0   \n",
      "9248     0.7   923.000000      -200   2.451939     616.250000     83.3   \n",
      "1741     1.2  1153.250000      -200   9.183688     946.500000    107.0   \n",
      "\n",
      "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)          T         RH  \\\n",
      "7076    983.250000   -200.0    897.500000       612.50  11.425000  32.825000   \n",
      "3453   1063.250000     85.0   1324.750000       416.50  40.175000  17.725000   \n",
      "3387    657.750000    135.0   1752.000000      1105.50  28.525000  32.050000   \n",
      "5043    541.500000   -200.0   1758.750000      1490.25  22.375000  57.850000   \n",
      "8244   1110.500000     51.4    794.250000       488.50   6.900000  42.225000   \n",
      "9075    464.000000    194.3   1499.750000      1843.75  18.825000  37.325000   \n",
      "2499    856.750000     93.0   1639.750000       948.75  25.250000  36.375000   \n",
      "1180    845.000000     96.0   1574.750000      1165.75  19.575000  49.800000   \n",
      "580     761.750000   -200.0   1869.750000      1207.00  18.150000  51.300000   \n",
      "6900   1096.250000     78.0    847.000000       831.00   2.800000  55.174999   \n",
      "1063    707.000000    100.0   1763.000000      1442.00  16.125000  57.449999   \n",
      "8121    685.000000   -200.0   1050.500000      1467.25   4.950000  66.250000   \n",
      "2539    938.500000     55.0   1576.000000       570.25  32.775002  28.900000   \n",
      "8686    886.250000    167.4    818.750000       682.00   9.675000  26.825000   \n",
      "3174    732.500000     89.0   1682.750000      1192.50  28.275000  36.775000   \n",
      "676    1152.750000     85.0   1390.750000       630.50   9.250000  75.224998   \n",
      "4474   1026.750000   -200.0   1435.500000       809.25  19.450000  73.424999   \n",
      "6773    818.750000    106.0   1231.750000       862.00  12.650000  74.324999   \n",
      "1612   1030.250000     97.0   1416.250000       750.00  20.750000  36.250000   \n",
      "3003    736.500000    128.0   1665.500000      1194.75  26.050000  28.550000   \n",
      "4811    888.500000     71.0   1266.250000       907.50  14.050000  61.700000   \n",
      "8531   1727.000000     29.9    560.500000       224.75  -0.250000  38.350000   \n",
      "781    1613.000000     53.0   1184.750000       367.00  13.475000  50.099999   \n",
      "1449   1462.000000   -200.0   1317.500000       488.75  13.850000  63.174999   \n",
      "2626   1079.000000     39.0   1438.000000       634.00  24.000000  46.550000   \n",
      "8746    837.500000     94.1   1033.500000       968.50   4.050000  80.850000   \n",
      "3814   1014.500000     60.0   1346.000000       520.25  39.724999  15.325000   \n",
      "848    1242.750000   -200.0   1259.000000       815.25  10.650000  62.324999   \n",
      "5602    826.333333     72.0   1372.333333       833.00  16.900000  79.800001   \n",
      "614     569.750000   -200.0   2298.500000      1614.50  14.475000  58.750000   \n",
      "...            ...      ...           ...          ...        ...        ...   \n",
      "3186    736.250000    138.0   1570.500000       980.00  38.925001  15.900000   \n",
      "7852    736.500000    208.0    960.000000      1526.50   5.375000  43.450001   \n",
      "8633    869.750000    116.8    955.000000       586.25  13.025000  30.250000   \n",
      "5294    961.250000   -200.0   1295.500000       591.00  15.850000  61.025001   \n",
      "5633    627.500000     77.0   1643.000000      1143.50  22.950000  64.575001   \n",
      "8855    540.000000    177.3   1440.500000      1319.25  18.900000  34.400001   \n",
      "4233    921.250000   -200.0   1355.750000       764.00  25.175000  47.950000   \n",
      "2401    673.000000    180.0   2043.000000      1167.50  30.475000  29.450001   \n",
      "4388    872.750000    168.0   1521.750000      1161.50  35.050000  17.550000   \n",
      "6633    832.250000   -200.0   1034.750000      1023.75   6.575000  58.100000   \n",
      "8553   1294.000000   -200.0    650.250000       419.75  -1.125000  41.350001   \n",
      "5444    609.250000    101.0   1608.000000      1269.50  26.125000  47.125000   \n",
      "692     844.750000    128.0   1550.750000      1009.50  20.650000  22.575000   \n",
      "8193   1235.250000   -200.0    762.250000       347.50   7.150000  37.350000   \n",
      "1419    835.250000   -200.0   1602.750000      1017.50  14.150000  56.225000   \n",
      "1867   1009.000000   -200.0   1574.250000       853.75  27.500000  20.925000   \n",
      "2344    764.750000    115.0   1878.750000      1061.25  27.100000  38.800000   \n",
      "6910    565.250000    177.0   1325.000000      1639.75  11.350000  33.575001   \n",
      "3369   1086.500000   -200.0   1259.250000       543.25  25.050000  32.575000   \n",
      "2356    745.000000    107.0   1839.750000      1124.50  23.300000  43.400000   \n",
      "5908    929.500000    114.0   1171.500000       805.25  12.325000  73.424999   \n",
      "8394    900.250000    168.5   1020.250000       756.00   3.875000  79.525000   \n",
      "1353   1634.500000   -200.0   1291.500000       354.50  12.700000  71.650000   \n",
      "7393    421.000000    181.7   1901.750000      1740.00  14.125000  57.775000   \n",
      "8538   1180.500000     95.4    647.250000       377.75   3.325000  29.150000   \n",
      "5016    401.250000   -200.0   2403.250000      2003.50  27.050000  40.199999   \n",
      "3455    990.750000     84.0   1378.500000       460.50  38.500000  19.800000   \n",
      "784    1209.500000     78.0   1325.250000       554.50  12.950000  59.575001   \n",
      "9248    874.000000     63.7   1168.750000       790.00  13.950000  68.700000   \n",
      "1741    794.500000     80.0   1642.500000      1075.75  17.175000  52.900001   \n",
      "\n",
      "            AH  \n",
      "7076  0.442442  \n",
      "3453  1.298945  \n",
      "3387  1.229571  \n",
      "5043  1.544922  \n",
      "8244  0.422400  \n",
      "9075  0.803105  \n",
      "2499  1.152813  \n",
      "1180  1.122057  \n",
      "580   1.058742  \n",
      "6900  0.417728  \n",
      "1063  1.045095  \n",
      "8121  0.581079  \n",
      "2539  1.412032  \n",
      "8686  0.322553  \n",
      "3174  1.390620  \n",
      "676   0.879605  \n",
      "4474  1.641728  \n",
      "6773  1.084272  \n",
      "1612  0.877490  \n",
      "3003  0.948412  \n",
      "4811  0.984439  \n",
      "8531  0.234715  \n",
      "781   0.770567  \n",
      "1449  0.995210  \n",
      "2626  1.370015  \n",
      "8746  0.666963  \n",
      "3814  1.096318  \n",
      "848   0.798782  \n",
      "5602  1.523823  \n",
      "614   0.963042  \n",
      "...        ...  \n",
      "3186  1.089553  \n",
      "7852  0.392241  \n",
      "8633  0.452049  \n",
      "5294  1.091117  \n",
      "5633  1.785064  \n",
      "8855  0.743596  \n",
      "4233  1.512945  \n",
      "2401  1.263462  \n",
      "4388  0.973308  \n",
      "6633  0.568681  \n",
      "8553  0.237886  \n",
      "5444  1.572362  \n",
      "692   0.543151  \n",
      "8193  0.379935  \n",
      "1419  0.902811  \n",
      "1867  0.756516  \n",
      "2344  1.370499  \n",
      "6910  0.450355  \n",
      "3369  1.020268  \n",
      "2356  1.225103  \n",
      "5908  1.048969  \n",
      "8394  0.648228  \n",
      "1353  1.048614  \n",
      "7393  0.926225  \n",
      "8538  0.228815  \n",
      "5016  1.415819  \n",
      "3455  1.326023  \n",
      "784   0.886003  \n",
      "9248  1.089166  \n",
      "1741  1.027627  \n",
      "\n",
      "[6550 rows x 13 columns]\n",
      "      CO(GT)  PT08.S1(CO)  NMHC(GT)    C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
      "381      1.3       961.75        81    5.252378         776.00     89.0   \n",
      "6613     1.2       955.25      -200    5.176520         772.25    243.0   \n",
      "3923  -200.0       815.50      -200    3.011833         652.75   -200.0   \n",
      "3011     0.8       855.25      -200    4.971584         762.00     79.0   \n",
      "929   -200.0      1216.50      -200    7.269933         868.50   -200.0   \n",
      "4080     2.0      -200.00      -200 -200.000000        -200.00    101.0   \n",
      "4987  -200.0      1282.00      -200   18.243426        1246.50   -200.0   \n",
      "816      4.6      1441.50       669   21.469617        1336.75    243.0   \n",
      "366      1.7      1047.50        88    5.477599         787.00     93.0   \n",
      "2090     1.8       988.75      -200    8.166235         906.00    118.0   \n",
      "6233     2.8      1110.50      -200   11.767010        1041.50    603.0   \n",
      "1668     0.7       980.25      -200    5.748950         800.00     46.0   \n",
      "5863     0.7       842.75      -200    2.533157         621.75   -200.0   \n",
      "7840     3.5      1293.00      -200   15.140161        1153.00    584.4   \n",
      "9296     0.6       835.25      -200    1.519090         546.25    101.8   \n",
      "5540     3.6      1339.75      -200   18.563760        1255.75    556.0   \n",
      "7921     6.4      1654.75      -200   29.000063        1527.25    965.9   \n",
      "7520     1.0       925.00      -200    3.410629         677.00    151.3   \n",
      "1244     1.3       963.25      -200    6.512663         835.25     59.0   \n",
      "4089     0.6       834.50      -200    3.790555         699.00   -200.0   \n",
      "7693  -200.0       762.00      -200    0.442990         434.00     77.2   \n",
      "6012     0.5       777.50      -200    1.186720         517.00     96.0   \n",
      "7111     1.6      1274.50      -200    8.763027         930.00    214.9   \n",
      "4631     4.3      1302.75      -200   20.735409        1316.75    329.0   \n",
      "976   -200.0      1348.25      -200   17.136842        1214.00   -200.0   \n",
      "6431     6.9      1647.50      -200   32.644660        1611.75    858.0   \n",
      "3490  -200.0       866.00      -200    2.264475         603.25     48.0   \n",
      "2847     4.0      1358.75      -200   24.455875        1415.25    381.0   \n",
      "5682     4.9      1426.00      -200   25.620709        1444.75    667.0   \n",
      "5774     2.8      1231.25      -200   14.524226        1133.50    448.0   \n",
      "...      ...          ...       ...         ...            ...      ...   \n",
      "2040  -200.0      1043.00      -200   14.681118        1138.50   -200.0   \n",
      "8209     2.8      1125.00      -200   14.127410        1120.75    357.2   \n",
      "8271     2.5      1186.50      -200    9.790648         969.75    477.0   \n",
      "1209     1.0      1053.75        71    4.951779         761.00   -200.0   \n",
      "4456  -200.0      1126.25      -200    6.385320         829.50   -200.0   \n",
      "7895     3.1      1112.75      -200   13.362988        1095.75    464.8   \n",
      "2985     0.4       722.75      -200    1.486361         543.50   -200.0   \n",
      "178   -200.0       954.00        28    2.884931         644.75     60.0   \n",
      "8027     0.6       910.00      -200    1.265835         524.25     74.3   \n",
      "7300     2.4      1198.00      -200    9.080938         942.50    371.3   \n",
      "8728     3.8      1374.00      -200   18.529015        1254.75    732.8   \n",
      "6769     3.2      -200.00      -200 -200.000000        -200.00    418.0   \n",
      "1875  -200.0      1109.75      -200   13.280011        1093.00   -200.0   \n",
      "7800     1.4       908.00      -200    3.011833         652.75    165.1   \n",
      "7758     1.2       894.50      -200    3.414851         677.25    143.1   \n",
      "6757     2.5      -200.00      -200 -200.000000        -200.00    658.0   \n",
      "4045     1.9      -200.00      -200 -200.000000        -200.00    121.0   \n",
      "4500  -200.0      1086.50      -200    9.632429         963.75   -200.0   \n",
      "275      0.3       845.00      -200    0.747676         472.25     15.0   \n",
      "4862     4.2      1325.50      -200   28.008952        1503.50    600.0   \n",
      "1823  -200.0      1149.50      -200   15.379958        1160.50   -200.0   \n",
      "2640     3.4      1255.25      -200   19.530103        1283.25    203.0   \n",
      "3435     2.2      1127.50      -200   12.957760        1082.25     95.0   \n",
      "2901     1.5       989.25      -200   10.332725         990.00    113.0   \n",
      "1311     3.2      1308.50      -200   16.207247        1186.00    192.0   \n",
      "3090     1.5      1019.25      -200    8.332268         912.75     78.0   \n",
      "8439     5.8      1526.50      -200   27.946849        1502.00   -200.0   \n",
      "2286     0.7       784.75      -200    1.965171         581.50     33.0   \n",
      "1526     3.6      1347.25      -200   17.899796        1236.50    264.0   \n",
      "7822     1.0       844.50      -200    1.794978         568.50    128.1   \n",
      "\n",
      "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)           T  \\\n",
      "381        1151.50     75.0       1279.00       549.75   14.175000   \n",
      "6613       1030.25    106.0        957.25       960.25    9.325000   \n",
      "3923       1057.75   -200.0       1332.25       486.25   24.750000   \n",
      "3011        941.75     73.0       1444.00       958.50   18.775000   \n",
      "929         872.75   -200.0       1660.25       963.25   14.875000   \n",
      "4080       -200.00    107.0       -200.00      -200.00 -200.000000   \n",
      "4987        560.50   -200.0       1829.25      1285.00   26.000000   \n",
      "816         629.50    134.0       2004.00      1523.00   15.275000   \n",
      "366        1098.25     97.0       1372.75       834.50    9.625000   \n",
      "2090        954.25    102.0       1681.00       826.00   23.450000   \n",
      "6233        681.50    171.0       1256.50      1434.00   13.600000   \n",
      "1668        983.25     54.0       1461.50       809.50   16.350000   \n",
      "5863       1145.50   -200.0       1060.25       554.50    6.925000   \n",
      "7840        601.75    283.2       1141.00      1667.25    6.550000   \n",
      "9296       1110.50     80.8        768.25       391.25   12.200000   \n",
      "5540        536.50    119.0       1842.25      1297.00   18.475000   \n",
      "7921        392.75    249.0       1757.75      2203.00   10.325000   \n",
      "7520        957.50    105.1        900.50      1023.50    3.200000   \n",
      "1244       1028.50     68.0       1495.25       555.75   24.775001   \n",
      "4089        911.50   -200.0       1360.50       796.00   19.750000   \n",
      "7693       1556.25     60.4        762.50       318.25    3.900000   \n",
      "6012       1330.25     63.0        889.50       510.00   10.150000   \n",
      "7111        648.50    106.3       1023.50      1617.00    5.325000   \n",
      "4631        630.50    151.0       1816.25      1196.25   33.200001   \n",
      "976         678.00   -200.0       1874.25      1508.75   16.050000   \n",
      "6431        401.00    173.0       2176.75      1885.00   17.700000   \n",
      "3490       1077.00     55.0       1458.75       588.50   25.750000   \n",
      "2847        571.50    189.0       2080.50      1902.25   29.575000   \n",
      "5682        477.00    151.0       1969.50      1782.25   23.950000   \n",
      "5774        587.00     87.0       1572.25      1142.50   14.600000   \n",
      "...            ...      ...           ...          ...         ...   \n",
      "2040        876.25   -200.0       1755.50      1001.25   27.150000   \n",
      "8209        630.25    184.9       1132.25      1404.75    9.500000   \n",
      "8271        625.25    198.9       1174.00      1203.00    8.375000   \n",
      "1209        962.75   -200.0       1597.75       964.50   15.675000   \n",
      "4456        852.50   -200.0       1527.75       937.50   27.875000   \n",
      "7895        644.50    157.5       1092.25      1358.75   13.175000   \n",
      "2985       1274.00   -200.0       1314.50       583.75   18.825000   \n",
      "178        1260.25     78.0       1334.00       924.75   11.600000   \n",
      "8027       1102.25     68.6        801.50       600.25    6.375000   \n",
      "7300        646.75    151.2       1307.25      1290.75   13.500000   \n",
      "8728        503.50    214.8       1375.00      1855.00    8.300000   \n",
      "6769       -200.00    129.0       -200.00      -200.00 -200.000000   \n",
      "1875        961.00   -200.0       1660.75      1090.75   21.350000   \n",
      "7800        979.75    117.7        846.75       551.00    3.300000   \n",
      "7758        998.50    111.1        827.25       699.75    3.675000   \n",
      "6757       -200.00    122.0       -200.00      -200.00 -200.000000   \n",
      "4045       -200.00     83.0       -200.00      -200.00 -200.000000   \n",
      "4500        774.25   -200.0       1632.75      1103.00   21.350000   \n",
      "275        1785.75     23.0       1278.00       378.25   16.525000   \n",
      "4862        510.25    122.0       1968.00      1609.25   15.125000   \n",
      "1823        866.50   -200.0       1512.00      1038.25   32.050000   \n",
      "2640        640.25    160.0       2022.50      1232.00   37.000000   \n",
      "3435        675.25    105.0       1817.00      1066.00   30.075000   \n",
      "2901        822.00    138.0       1577.25       705.00   33.200001   \n",
      "1311        648.50    104.0       1973.00      1307.75   17.650000   \n",
      "3090        839.50     94.0       1427.50       758.50   36.875000   \n",
      "8439        410.50   -200.0       1867.75      1726.75    6.250000   \n",
      "2286       1376.75     45.0       1311.00       416.50   19.425000   \n",
      "1526        645.25    136.0       1984.25      1328.25   16.025000   \n",
      "7822       1150.50     94.9        709.00       396.75    7.925000   \n",
      "\n",
      "              RH          AH  \n",
      "381    36.825000    0.592244  \n",
      "6613   43.075000    0.506169  \n",
      "3923   43.374999    1.334651  \n",
      "3011   48.974999    1.050532  \n",
      "929    74.525000    1.253001  \n",
      "4080 -200.000000 -200.000000  \n",
      "4987   44.475000    1.473103  \n",
      "816    46.825001    0.807437  \n",
      "366    61.100000    0.732280  \n",
      "2090   44.400000    1.264609  \n",
      "6233   48.675000    0.754653  \n",
      "1668   52.099999    0.961238  \n",
      "5863   82.275002    0.824422  \n",
      "7840   34.375000    0.335897  \n",
      "9296   35.550000    0.503800  \n",
      "5540   76.200001    1.604549  \n",
      "7921   52.425000    0.657796  \n",
      "7520   58.825000    0.457800  \n",
      "1244   34.774999    1.071612  \n",
      "4089   59.299999    1.350502  \n",
      "7693   47.300000    0.386214  \n",
      "6012   43.575001    0.540530  \n",
      "7111   50.725000    0.456368  \n",
      "4631   25.425000    1.272183  \n",
      "976    56.250000    1.018459  \n",
      "6431   68.324999    1.371320  \n",
      "3490   50.875000    1.660524  \n",
      "2847   28.725000    1.170595  \n",
      "5682   50.025001    1.467915  \n",
      "5774   74.175001    1.225574  \n",
      "...          ...         ...  \n",
      "2040   28.275000    1.001646  \n",
      "8209   25.200001    0.299550  \n",
      "8271   44.599999    0.492220  \n",
      "1209   77.375000    1.368304  \n",
      "4456   43.350000    1.601737  \n",
      "7895   20.375000    0.307421  \n",
      "2985   51.125000    1.100034  \n",
      "178    61.924999    0.844196  \n",
      "8027   42.724999    0.412611  \n",
      "7300   62.599999    0.964364  \n",
      "8728   43.474999    0.477425  \n",
      "6769 -200.000000 -200.000000  \n",
      "1875   32.999999    0.828433  \n",
      "7800   46.849999    0.367121  \n",
      "7758   46.025001    0.370058  \n",
      "6757 -200.000000 -200.000000  \n",
      "4045 -200.000000 -200.000000  \n",
      "4500   59.425000    1.491808  \n",
      "275    53.874999    1.004939  \n",
      "4862   59.049999    1.008640  \n",
      "1823   10.650000    0.499560  \n",
      "2640   21.875000    1.350342  \n",
      "3435   37.675000    1.579872  \n",
      "2901   20.375000    1.019497  \n",
      "1311   70.825001    1.417091  \n",
      "3090   17.250000    1.057597  \n",
      "8439   76.799999    0.735477  \n",
      "2286   45.875000    1.024159  \n",
      "1526   59.250000    1.071094  \n",
      "7822   25.900000    0.277430  \n",
      "\n",
      "[2807 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import numpy as np\n",
    "# The function splitData divide data set\n",
    "def splitData(data,seed,rate):\n",
    "    np.random.seed(seed)\n",
    "    data_index = np.random.permutation(data.index)\n",
    "    data = data.reindex(data_index)\n",
    "    num = int(len(data_index)*rate)\n",
    "    data_train = data.iloc[num:,:]\n",
    "    data_test = data.iloc[:num,:]\n",
    "    return (data_train,data_test)\n",
    "       \n",
    "splitData_train,splitData_test = splitData(air_quality,18979,0.3)# Returns 70% for the training set and 30% for the test set\n",
    "print(splitData_train)\n",
    "print(splitData_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "The dataset has missing values tagged with a -200 value. Before doing any work with the training data, we want to make sure that we deal properly with the missing values. \n",
    "\n",
    "### Question 4: Missing values (3 marks)\n",
    "\n",
    "Make some exploratory analysis on the number of missing values per column in the training data. \n",
    "\n",
    "* Remove the rows for which the target feature has missing values. We are doing supervised learning so we need all our data observations to have known target values.\n",
    "\n",
    "* Remove features with more than 20% of missing values. For all the other features with missing values, use the mean value of the non-missing values for imputation.\n",
    "\n",
    "#### Question 4 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "#Remove the rows for which the target feature has missing values\n",
    "splitData_train_new = splitData_train[splitData_train['CO(GT)'] != -200]\n",
    "#print(splitData_train_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5356 entries, 3453 to 1741\n",
      "Data columns (total 13 columns):\n",
      "CO(GT)           5356 non-null float64\n",
      "PT08.S1(CO)      5356 non-null float64\n",
      "NMHC(GT)         5356 non-null int64\n",
      "C6H6(GT)         5356 non-null float64\n",
      "PT08.S2(NMHC)    5356 non-null float64\n",
      "NOx(GT)          5356 non-null float64\n",
      "PT08.S3(NOx)     5356 non-null float64\n",
      "NO2(GT)          5356 non-null float64\n",
      "PT08.S4(NO2)     5356 non-null float64\n",
      "PT08.S5(O3)      5356 non-null float64\n",
      "T                5356 non-null float64\n",
      "RH               5356 non-null float64\n",
      "AH               5356 non-null float64\n",
      "dtypes: float64(12), int64(1)\n",
      "memory usage: 585.8 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CO(GT)              0\n",
       "PT08.S1(CO)       229\n",
       "NMHC(GT)         4743\n",
       "C6H6(GT)          229\n",
       "PT08.S2(NMHC)     229\n",
       "NOx(GT)           286\n",
       "PT08.S3(NOx)      229\n",
       "NO2(GT)           288\n",
       "PT08.S4(NO2)      229\n",
       "PT08.S5(O3)       229\n",
       "T                 229\n",
       "RH                229\n",
       "AH                229\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove features with more than 20% of missing values\n",
    "splitData_train_new.info()# View the number of rows per feature\n",
    "splitData_train_new = splitData_train_new.replace(to_replace = -200, value = np.NaN,inplace = False)# Replace the value with a characteristic of -200 with a null value\n",
    "splitData_train_new.isnull().sum()# Returns the number of null values for each feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CO(GT)  PT08.S1(CO)   C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  \\\n",
      "3453     0.7       868.25   3.937417         707.25     63.0       1063.25   \n",
      "8244     0.5       893.00   1.076022         506.50     59.4       1110.50   \n",
      "9075     3.9      1445.00  18.825234        1263.25    550.5        464.00   \n",
      "2499     1.8       958.75   9.254615         949.25    104.0        856.75   \n",
      "1180     1.4      1145.75   8.587110         923.00     79.0        845.00   \n",
      "6900     1.2       874.25   2.548045         622.75    125.0       1096.25   \n",
      "1063     2.6      1348.50  13.400779        1097.00    148.0        707.00   \n",
      "8121     2.5      1185.25   5.902703         807.25      NaN        685.00   \n",
      "2539     1.1       974.75   5.706840         798.00     51.0        938.50   \n",
      "8686     1.4       906.75   4.789679         752.75    313.2        886.25   \n",
      "676      1.3      1020.25   4.293956         726.75     81.0       1152.75   \n",
      "6773     1.4      1060.75   5.738410         799.50    200.0        818.75   \n",
      "1612     1.4      1033.75   5.860116         805.25     92.0       1030.25   \n",
      "3003     2.6      1029.25  13.628473        1104.50    203.0        736.50   \n",
      "4811     0.9       893.50   5.628240         794.25    179.0        888.50   \n",
      "8531     0.2       730.75   0.274336         407.75     32.2       1727.00   \n",
      "781      0.5       865.00   0.945425         493.50     39.0       1613.00   \n",
      "2626     0.4       837.75   2.691331         632.25     24.0       1079.00   \n",
      "8746     1.0       987.50   3.347581         673.25    164.7        837.50   \n",
      "3814     1.0       871.00   4.956727         761.25     61.0       1014.50   \n",
      "5141     1.5      1073.25   6.579528         838.25    178.0        768.25   \n",
      "3678     1.5      1118.25   8.769338         930.25     65.0        697.25   \n",
      "3020     2.0       965.50  14.438242        1130.75    200.0        799.75   \n",
      "7958     3.1      1262.50  12.705350        1073.75    708.0        613.75   \n",
      "7580     3.0      1093.50  14.336906        1127.50    438.2        625.75   \n",
      "4021     1.9      1173.00  11.910476        1046.50    160.0        630.50   \n",
      "8225     1.3       955.50   4.951779         761.00    205.1        849.25   \n",
      "6688     5.6      1521.25  30.713161        1567.50    920.0        440.00   \n",
      "5445     3.1      1324.00  15.766969        1172.50    365.0        563.50   \n",
      "5568     5.9      1463.25  28.185228        1507.75    650.0        467.75   \n",
      "...      ...          ...        ...            ...      ...           ...   \n",
      "767      1.2       935.25   3.322503         671.75     65.0       1298.75   \n",
      "4553     1.3       981.75   6.849892         850.25    217.0        887.50   \n",
      "172      2.8      1443.75  15.044693        1150.00    202.0        769.50   \n",
      "2205     2.3      1141.75  13.750579        1108.50    140.0        781.50   \n",
      "7140     1.0       910.50   2.408114         613.25    148.4        991.50   \n",
      "4744     3.4      1337.50  20.300122        1304.75    539.0        543.25   \n",
      "5875     1.9       995.75   7.838454         892.50    391.0        808.00   \n",
      "7852     2.1      1109.00   7.148008         863.25    359.7        736.50   \n",
      "8633     1.1       977.75   5.171480         772.00    161.6        869.75   \n",
      "5633     2.4      1239.75  10.977346        1013.50    303.0        627.50   \n",
      "8855     3.4      1324.00  14.360264        1128.25    492.3        540.00   \n",
      "2401     3.5      1215.75  20.780977        1318.00    218.0        673.00   \n",
      "4388     2.5      1159.75  16.182652        1185.25    396.0        872.75   \n",
      "6633     2.0       973.75   6.446089         832.25      NaN        832.25   \n",
      "8553     0.5       787.75   0.875741         486.25      NaN       1294.00   \n",
      "5444     2.3      1211.25  12.513760        1067.25    230.0        609.25   \n",
      "692      2.8      1144.00  13.903863        1113.50    181.0        844.75   \n",
      "8193     0.4       821.25   1.104567         509.25      NaN       1235.25   \n",
      "2344     2.4      1125.75  12.243257        1058.00    168.0        764.75   \n",
      "6910     4.3      1305.00  17.942598        1237.75    587.0        565.25   \n",
      "2356     2.5      1066.75  13.674208        1106.00    143.0        745.00   \n",
      "5908     1.2       903.50   4.809205         753.75    193.0        929.50   \n",
      "8394     1.2       992.25   3.693973         693.50    265.8        900.25   \n",
      "1353     0.4       742.50   1.030001         502.00      NaN       1634.50   \n",
      "7393     5.7      1516.75  28.351566        1511.75    728.4        421.00   \n",
      "8538     0.7       782.50   1.941878         579.75    174.1       1180.50   \n",
      "3455     0.9       912.25   5.257451         776.25     61.0        990.75   \n",
      "784      1.0      1040.25   2.900667         645.75     71.0       1209.50   \n",
      "9248     0.7       923.00   2.451939         616.25     83.3        874.00   \n",
      "1741     1.2      1153.25   9.183688         946.50    107.0        794.50   \n",
      "\n",
      "      NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)          T         RH        AH  \n",
      "3453     85.0       1324.75       416.50  40.175000  17.725000  1.298945  \n",
      "8244     51.4        794.25       488.50   6.900000  42.225000  0.422400  \n",
      "9075    194.3       1499.75      1843.75  18.825000  37.325000  0.803105  \n",
      "2499     93.0       1639.75       948.75  25.250000  36.375000  1.152813  \n",
      "1180     96.0       1574.75      1165.75  19.575000  49.800000  1.122057  \n",
      "6900     78.0        847.00       831.00   2.800000  55.174999  0.417728  \n",
      "1063    100.0       1763.00      1442.00  16.125000  57.449999  1.045095  \n",
      "8121      NaN       1050.50      1467.25   4.950000  66.250000  0.581079  \n",
      "2539     55.0       1576.00       570.25  32.775002  28.900000  1.412032  \n",
      "8686    167.4        818.75       682.00   9.675000  26.825000  0.322553  \n",
      "676      85.0       1390.75       630.50   9.250000  75.224998  0.879605  \n",
      "6773    106.0       1231.75       862.00  12.650000  74.324999  1.084272  \n",
      "1612     97.0       1416.25       750.00  20.750000  36.250000  0.877490  \n",
      "3003    128.0       1665.50      1194.75  26.050000  28.550000  0.948412  \n",
      "4811     71.0       1266.25       907.50  14.050000  61.700000  0.984439  \n",
      "8531     29.9        560.50       224.75  -0.250000  38.350000  0.234715  \n",
      "781      53.0       1184.75       367.00  13.475000  50.099999  0.770567  \n",
      "2626     39.0       1438.00       634.00  24.000000  46.550000  1.370015  \n",
      "8746     94.1       1033.50       968.50   4.050000  80.850000  0.666963  \n",
      "3814     60.0       1346.00       520.25  39.724999  15.325000  1.096318  \n",
      "5141     63.0       1548.50       892.00  20.700000  68.400000  1.650706  \n",
      "3678     60.0       1747.50      1026.50  28.175000  55.049999  2.069675  \n",
      "3020    127.0       1612.25       967.50  35.674999  13.650000  0.783575  \n",
      "7958    282.6       1198.25      1403.00   2.025000  63.600001  0.456383  \n",
      "7580    153.8       1191.00      1279.75  14.800000  25.450000  0.425868  \n",
      "4021     89.0       1815.00      1174.75  24.900001  52.925000  1.643016  \n",
      "8225     98.1        887.25       884.50   9.100000  33.049999  0.382653  \n",
      "6688    192.0       1842.50      2097.00   9.325000  64.200000  0.754406  \n",
      "5445    119.0       1748.75      1354.25  26.975000  44.175000  1.549030  \n",
      "5568    136.0       2092.00      1669.75  20.250000  66.675000  1.565572  \n",
      "...       ...           ...          ...        ...        ...       ...  \n",
      "767      61.0       1182.75       414.50  19.575000  27.350000  0.616230  \n",
      "4553     79.0       1504.50       873.00  18.700000  73.100000  1.560790  \n",
      "172     136.0       1726.50      1726.50  17.150000  44.125001  0.855831  \n",
      "2205    133.0       1788.75      1047.00  40.424999  17.200000  1.277426  \n",
      "7140     81.7        934.00       861.25   2.375000  71.924999  0.528788  \n",
      "4744    138.0       1929.00      1520.50  25.125000  49.500001  1.557249  \n",
      "5875    175.0       1237.25       894.75   9.450000  70.025000  0.829649  \n",
      "7852    208.0        960.00      1526.50   5.375000  43.450001  0.392241  \n",
      "8633    116.8        955.00       586.25  13.025000  30.250000  0.452049  \n",
      "5633     77.0       1643.00      1143.50  22.950000  64.575001  1.785064  \n",
      "8855    177.3       1440.50      1319.25  18.900000  34.400001  0.743596  \n",
      "2401    180.0       2043.00      1167.50  30.475000  29.450001  1.263462  \n",
      "4388    168.0       1521.75      1161.50  35.050000  17.550000  0.973308  \n",
      "6633      NaN       1034.75      1023.75   6.575000  58.100000  0.568681  \n",
      "8553      NaN        650.25       419.75  -1.125000  41.350001  0.237886  \n",
      "5444    101.0       1608.00      1269.50  26.125000  47.125000  1.572362  \n",
      "692     128.0       1550.75      1009.50  20.650000  22.575000  0.543151  \n",
      "8193      NaN        762.25       347.50   7.150000  37.350000  0.379935  \n",
      "2344    115.0       1878.75      1061.25  27.100000  38.800000  1.370499  \n",
      "6910    177.0       1325.00      1639.75  11.350000  33.575001  0.450355  \n",
      "2356    107.0       1839.75      1124.50  23.300000  43.400000  1.225103  \n",
      "5908    114.0       1171.50       805.25  12.325000  73.424999  1.048969  \n",
      "8394    168.5       1020.25       756.00   3.875000  79.525000  0.648228  \n",
      "1353      NaN       1291.50       354.50  12.700000  71.650000  1.048614  \n",
      "7393    181.7       1901.75      1740.00  14.125000  57.775000  0.926225  \n",
      "8538     95.4        647.25       377.75   3.325000  29.150000  0.228815  \n",
      "3455     84.0       1378.50       460.50  38.500000  19.800000  1.326023  \n",
      "784      78.0       1325.25       554.50  12.950000  59.575001  0.886003  \n",
      "9248     63.7       1168.75       790.00  13.950000  68.700000  1.089166  \n",
      "1741     80.0       1642.50      1075.75  17.175000  52.900001  1.027627  \n",
      "\n",
      "[5356 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#Drop features NMHC_GT, it has more than 20% of missing values\n",
    "del splitData_train_new['NMHC(GT)']\n",
    "print(splitData_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CO(GT)           0\n",
       "PT08.S1(CO)      0\n",
       "C6H6(GT)         0\n",
       "PT08.S2(NMHC)    0\n",
       "NOx(GT)          0\n",
       "PT08.S3(NOx)     0\n",
       "NO2(GT)          0\n",
       "PT08.S4(NO2)     0\n",
       "PT08.S5(O3)      0\n",
       "T                0\n",
       "RH               0\n",
       "AH               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For all the other features with missing values, use the mean value of the non-missing values for imputation.\n",
    "for i in range(12):\n",
    "    splitData_train_new[splitData_train_new.columns[i]].fillna(splitData_train_new[splitData_train_new.columns[i]].mean(), inplace=True)\n",
    "\n",
    "splitData_train_new.isnull().sum()# We can see that each feature has no null value now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Normalising the training data (2 marks)\n",
    "\n",
    "Now that you have removed the missing data, we need to normalise the input vectors. \n",
    "\n",
    "* Explain in a sentence why do you need to normalise the input features for this dataset.\n",
    "\n",
    "* Normalise the training data by substracting the mean value for each feature and dividing the result by the standard deviation of each feature. Keep the mean values and standard deviations, you will need them at test time.\n",
    "\n",
    "#### Question 5 Answer\n",
    "\n",
    "Write your explanation in this box\n",
    "\n",
    "The main role of normalizition is to increase the speed of iterations, and reduce the inconsistency of the influence weights between different features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO(GT)              2.157207\n",
      "PT08.S1(CO)      1111.229358\n",
      "C6H6(GT)           10.316962\n",
      "PT08.S2(NMHC)     948.362493\n",
      "NOx(GT)           256.401006\n",
      "PT08.S3(NOx)      826.580733\n",
      "NO2(GT)           114.768804\n",
      "PT08.S4(NO2)     1445.875788\n",
      "PT08.S5(O3)      1044.470954\n",
      "T                  17.736326\n",
      "RH                 49.189437\n",
      "AH                  0.990122\n",
      "dtype: float64\n",
      "CO(GT)             1.465612\n",
      "PT08.S1(CO)      214.447415\n",
      "C6H6(GT)           7.347290\n",
      "PT08.S2(NMHC)    260.876917\n",
      "NOx(GT)          209.774932\n",
      "PT08.S3(NOx)     253.285394\n",
      "NO2(GT)           46.816541\n",
      "PT08.S4(NO2)     342.832259\n",
      "PT08.S5(O3)      399.494012\n",
      "T                  8.625977\n",
      "RH                17.087079\n",
      "AH                 0.391276\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#mean values and standard deviations\n",
    "mean = splitData_train_new[splitData_train_new.columns].mean()\n",
    "std = splitData_train_new[splitData_train_new.columns].std()\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CO(GT)  PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)       NOx(GT)  \\\n",
      "3453     0.7    -1.133049 -0.868285      -0.924239 -9.219453e-01   \n",
      "8244     0.5    -1.017636 -1.257734      -1.693758 -9.391065e-01   \n",
      "9075     3.9     1.556422  1.158015       1.207035  1.401974e+00   \n",
      "2499     1.8    -0.711034 -0.144590       0.003402 -7.264977e-01   \n",
      "1180     1.4     0.160975 -0.235441      -0.097220 -8.456730e-01   \n",
      "6900     1.2    -1.105070 -1.057385      -1.248146 -6.263904e-01   \n",
      "1063     2.6     1.106428  0.419722       0.569761 -5.167491e-01   \n",
      "8121     2.5     0.345169 -0.600801      -0.540916 -8.129201e-16   \n",
      "2539     1.1    -0.636423 -0.627459      -0.576373 -9.791494e-01   \n",
      "8686     1.4    -0.953517 -0.752289      -0.749827  2.707616e-01   \n",
      "676      1.3    -0.424250 -0.819759      -0.849491 -8.361390e-01   \n",
      "6773     1.4    -0.235393 -0.623162      -0.570623 -2.688644e-01   \n",
      "1612     1.4    -0.361298 -0.606597      -0.548582 -7.837019e-01   \n",
      "3003     2.6    -0.382282  0.450712       0.598510 -2.545633e-01   \n",
      "4811     0.9    -1.015304 -0.638157      -0.590748 -3.689717e-01   \n",
      "8531     0.2    -1.774231 -1.366847      -2.072289 -1.068769e+00   \n",
      "781      0.5    -1.148204 -1.275509      -1.743590 -1.036354e+00   \n",
      "2626     0.4    -1.275275 -1.037883      -1.211730 -1.107859e+00   \n",
      "8746     1.0    -0.576968 -0.948565      -1.054568 -4.371400e-01   \n",
      "3814     1.0    -1.120225 -0.729553      -0.717244 -9.314793e-01   \n",
      "5141     1.5    -0.177103 -0.508682      -0.422086 -3.737387e-01   \n",
      "3678     1.5     0.032738 -0.210639      -0.069429 -9.124112e-01   \n",
      "3020     2.0    -0.679558  0.560925       0.699132 -2.688644e-01   \n",
      "7958     3.1     0.705397  0.325071       0.480639  2.152779e+00   \n",
      "7580     3.0    -0.082675  0.547133       0.686674  8.666383e-01   \n",
      "4021     1.9     0.288046  0.216885       0.376183 -4.595449e-01   \n",
      "8225     1.3    -0.726189 -0.730226      -0.718203 -2.445526e-01   \n",
      "6688     5.6     1.911987  2.776016       2.373294  3.163386e+00   \n",
      "5445     3.1     0.992181  0.741771       0.859170  5.176929e-01   \n",
      "5568     5.9     1.641524  2.431953       2.144258  1.876292e+00   \n",
      "...      ...          ...       ...            ...           ...   \n",
      "767      1.2    -0.820618 -0.951978      -1.060318 -9.124112e-01   \n",
      "4553     1.3    -0.603781 -0.471884      -0.376087 -1.878251e-01   \n",
      "172      2.8     1.550593  0.643466       0.772922 -2.593303e-01   \n",
      "2205     2.3     0.142322  0.467331       0.613843 -5.548852e-01   \n",
      "7140     1.0    -0.936031 -1.076430      -1.284562 -5.148423e-01   \n",
      "4744     3.4     1.055133  1.358754       1.366114  1.347153e+00   \n",
      "5875     1.9    -0.538497 -0.337336      -0.214134  6.416353e-01   \n",
      "7852     2.1    -0.010396 -0.431309      -0.326255  4.924277e-01   \n",
      "8633     1.1    -0.622434 -0.700324      -0.676037 -4.519177e-01   \n",
      "5633     2.4     0.599311  0.089881       0.249687  2.221380e-01   \n",
      "8855     3.4     0.992181  0.550312       0.689549  1.124534e+00   \n",
      "2401     3.5     0.487395  1.424201       1.416904 -1.830581e-01   \n",
      "4388     2.5     0.226259  0.798347       0.908043  6.654703e-01   \n",
      "6633     2.0    -0.641087 -0.526844      -0.445085 -8.129201e-16   \n",
      "8553     0.5    -1.508432 -1.284994      -1.771381 -8.129201e-16   \n",
      "5444     2.3     0.466411  0.298994       0.455723 -1.258540e-01   \n",
      "692      2.8     0.152814  0.488194       0.633009 -3.594376e-01   \n",
      "8193     0.4    -1.352217 -1.253849      -1.683217 -8.129201e-16   \n",
      "2344     2.4     0.067712  0.262178       0.420265 -4.214088e-01   \n",
      "6910     4.3     0.903581  1.037884       1.109288  1.575970e+00   \n",
      "2356     2.5    -0.207414  0.456937       0.604260 -5.405842e-01   \n",
      "5908     1.2    -0.968673 -0.749631      -0.745994 -3.022335e-01   \n",
      "8394     1.2    -0.554818 -0.901419      -0.976945  4.480513e-02   \n",
      "1353     0.4    -1.719440 -1.263998      -1.711008 -8.129201e-16   \n",
      "7393     5.7     1.891003  2.454593       2.159591  2.250026e+00   \n",
      "8538     0.7    -1.532914 -1.139887      -1.412975 -3.923300e-01   \n",
      "3455     0.9    -0.927870 -0.688623      -0.659746 -9.314793e-01   \n",
      "784      1.0    -0.330987 -1.009392      -1.159982 -8.838092e-01   \n",
      "9248     0.7    -0.877741 -1.070466      -1.273062 -8.251749e-01   \n",
      "1741     1.2     0.195948 -0.154244      -0.007139 -7.121967e-01   \n",
      "\n",
      "      PT08.S3(NOx)       NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)         T  \\\n",
      "3453      0.934398 -6.358608e-01     -0.353309    -1.571916  2.601291   \n",
      "8244      1.120946 -1.353556e+00     -1.900713    -1.391688 -1.256243   \n",
      "9075     -1.431511  1.698784e+00      0.157145     2.000728  0.126209   \n",
      "2499      0.119112 -4.649810e-01      0.565507    -0.239605  0.871052   \n",
      "1180      0.072721 -4.009011e-01      0.375910     0.303582  0.213156   \n",
      "6900      1.064685 -7.853806e-01     -1.746848    -0.534353 -1.731552   \n",
      "1063     -0.472119 -3.154612e-01      0.925013     0.995081 -0.186799   \n",
      "8121     -0.558977 -7.588586e-15     -1.153263     1.058286 -1.482305   \n",
      "2539      0.441870 -1.276660e+00      0.379556    -1.187054  1.743417   \n",
      "8686      0.235581  1.124201e+00     -1.829250    -0.907325 -0.934541   \n",
      "676       1.287754 -6.358608e-01     -0.160795    -1.036238 -0.983810   \n",
      "6773     -0.030917 -1.873014e-01     -0.624579    -0.456755 -0.589652   \n",
      "1612      0.804110 -3.795412e-01     -0.086415    -0.737110  0.349372   \n",
      "3003     -0.355649  2.826180e-01      0.640617     0.376173  0.963795   \n",
      "4811      0.244464 -9.349004e-01     -0.523947    -0.342861 -0.427352   \n",
      "8531      3.554959 -1.812795e+00     -2.582533    -2.051898 -2.085135   \n",
      "781       3.104874 -1.319380e+00     -0.761672    -1.695823 -0.494011   \n",
      "2626      0.996580 -1.618420e+00     -0.022973    -1.027477  0.726141   \n",
      "8746      0.043111 -4.414851e-01     -1.202850    -0.190168 -1.586641   \n",
      "3814      0.741927 -1.169860e+00     -0.291326    -1.312212  2.549123   \n",
      "5141     -0.230296 -1.105780e+00      0.299342    -0.381660  0.343576   \n",
      "3678     -0.510613 -1.169860e+00      0.879801    -0.044984  1.210144   \n",
      "3020     -0.105931  2.612580e-01      0.485293    -0.192671  2.079611   \n",
      "7958     -0.840280  3.584870e+00     -0.722294     0.897458 -1.821397   \n",
      "7580     -0.792903  8.337052e-01     -0.743442     0.588943 -0.340405   \n",
      "4021     -0.774149 -5.504209e-01      1.076690     0.326110  0.830477   \n",
      "8225      0.089501 -3.560452e-01     -1.629443    -0.400434 -1.001200   \n",
      "6688     -1.526265  1.649656e+00      1.156905     2.634655 -0.975116   \n",
      "5445     -1.038673  9.037822e-02      0.883447     0.775429  1.071029   \n",
      "5568     -1.416705  4.534977e-01      1.884666     1.565178  0.291407   \n",
      "...            ...           ...           ...          ...       ...   \n",
      "767       1.864179 -1.148500e+00     -0.767506    -1.576922  0.213155   \n",
      "4553      0.240516 -7.640207e-01      0.171000    -0.429220  0.111718   \n",
      "172      -0.225361  4.534977e-01      0.818547     1.707232 -0.067972   \n",
      "2205     -0.177984  3.894178e-01      1.000122     0.006331  2.630273   \n",
      "7140      0.651120 -7.063487e-01     -1.493079    -0.458633 -1.780821   \n",
      "4744     -1.118622  4.962177e-01      1.409215     1.191580  0.856561   \n",
      "5875     -0.073359  1.286537e+00     -0.608536    -0.374776 -0.960625   \n",
      "7852     -0.355649  1.991416e+00     -1.417241     1.206599 -1.433035   \n",
      "8633      0.170437  4.338628e-02     -1.431825    -1.147003 -0.546179   \n",
      "5633     -0.785994 -8.067406e-01      0.574987     0.247886  0.604416   \n",
      "8855     -1.131454  1.335665e+00     -0.015681     0.687818  0.134903   \n",
      "2401     -0.606354  1.393337e+00      1.741739     0.307962  1.476780   \n",
      "4388      0.182282  1.137017e+00      0.221316     0.292943  2.007155   \n",
      "6633      0.022383 -7.588586e-15     -1.199204    -0.051868 -1.293920   \n",
      "8553      1.845425 -7.588586e-15     -2.320744    -1.563781 -2.186573   \n",
      "5444     -0.858047 -2.941013e-01      0.472897     0.563285  0.972490   \n",
      "692       0.071734  2.826180e-01      0.305905    -0.087538  0.337779   \n",
      "8193      1.613473 -7.588586e-15     -1.994053    -1.744634 -1.227261   \n",
      "2344     -0.244115  4.938334e-03      1.262641     0.042001  1.085520   \n",
      "6910     -1.031764  1.329257e+00     -0.352580     1.490083 -0.740360   \n",
      "2356     -0.322090 -1.659414e-01      1.148883     0.200326  0.644991   \n",
      "5908      0.406337 -1.642164e-02     -0.800321    -0.598810 -0.627329   \n",
      "8394      0.290855  1.147697e+00     -1.241499    -0.722091 -1.606928   \n",
      "1353      3.189759 -7.588586e-15     -0.450295    -1.727112 -0.583856   \n",
      "7393     -1.601280  1.429648e+00      1.329730     1.741025 -0.418657   \n",
      "8538      1.397314 -4.137171e-01     -2.329494    -1.668914 -1.670689   \n",
      "3455      0.648159 -6.572208e-01     -0.196527    -1.461776  2.407110   \n",
      "784       1.511810 -7.853806e-01     -0.351851    -1.226479 -0.554873   \n",
      "9248      0.187217 -1.090828e+00     -0.808342    -0.636983 -0.438945   \n",
      "1741     -0.126658 -7.426607e-01      0.573529     0.078297 -0.065074   \n",
      "\n",
      "            RH        AH  \n",
      "3453 -1.841417  0.789272  \n",
      "8244 -0.407585 -1.450951  \n",
      "9075 -0.694351 -0.477967  \n",
      "2499 -0.749949  0.415796  \n",
      "1180  0.035732  0.337193  \n",
      "6900  0.350298 -1.462893  \n",
      "1063  0.483439  0.140497  \n",
      "8121  0.998448 -1.045409  \n",
      "2539 -1.187414  1.078294  \n",
      "8686 -1.308851 -1.706134  \n",
      "676   1.523699 -0.282452  \n",
      "6773  1.471027  0.240625  \n",
      "1612 -0.757264 -0.287859  \n",
      "3003 -1.207897 -0.106599  \n",
      "4811  0.732165 -0.014525  \n",
      "8531 -0.634365 -1.930626  \n",
      "781   0.053290 -0.561126  \n",
      "2626 -0.154470  0.970908  \n",
      "8746  1.852895 -0.825911  \n",
      "3814 -1.981874  0.271411  \n",
      "5141  1.124274  1.688284  \n",
      "3678  0.342982  2.759060  \n",
      "3020 -2.079901 -0.527880  \n",
      "7958  0.843360 -1.364099  \n",
      "7580 -1.389321 -1.442089  \n",
      "4021  0.218619  1.668630  \n",
      "8225 -0.944540 -1.552535  \n",
      "6688  0.878475 -0.602430  \n",
      "5445 -0.293464  1.428426  \n",
      "5568  1.023321  1.470702  \n",
      "...        ...       ...  \n",
      "767  -1.278126 -0.955571  \n",
      "4553  1.399336  1.458481  \n",
      "172  -0.296390 -0.343214  \n",
      "2205 -1.872142  0.734275  \n",
      "7140  1.330570 -1.179052  \n",
      "4744  0.018175  1.449432  \n",
      "5875  1.219375 -0.410128  \n",
      "7852 -0.335893 -1.528030  \n",
      "8633 -1.108407 -1.375176  \n",
      "5633  0.900421  2.031668  \n",
      "8855 -0.865533 -0.630058  \n",
      "2401 -1.155226  0.698588  \n",
      "4388 -1.851659 -0.042973  \n",
      "6633  0.521480 -1.077096  \n",
      "8553 -0.458793 -1.922524  \n",
      "5444 -0.120819  1.488057  \n",
      "692  -1.557577 -1.142343  \n",
      "8193 -0.692888 -1.559482  \n",
      "2344 -0.608029  0.972145  \n",
      "6910 -0.913815 -1.379506  \n",
      "2356 -0.338820  0.600551  \n",
      "5908  1.418356  0.150397  \n",
      "8394  1.775351 -0.873793  \n",
      "1353  1.314476  0.149490  \n",
      "7393  0.502459 -0.163304  \n",
      "8538 -1.172783 -1.945706  \n",
      "3455 -1.719980  0.858478  \n",
      "784   0.607802 -0.266101  \n",
      "9248  1.141831  0.253132  \n",
      "1741  0.217156  0.095852  \n",
      "\n",
      "[5356 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# Normalise the training data by substracting the mean value and dividing the result by the standard deviation of each feature\n",
    "for i in range(1,12):\n",
    "    splitData_train_new[splitData_train_new.columns[i]] = (splitData_train_new[splitData_train_new.columns[i]] - mean[i])/std[i]\n",
    "\n",
    "print(splitData_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation stages\n",
    "\n",
    "We have now curated our training data by removing data observations and features with a large amount of missing values. We have also normalised the feature vectors. We are now in a good position to work on developing the prediction model and validating it. We will use both the closed form expression for $\\mathbf{w}$ and gradient descent for iterative optimisation. \n",
    "\n",
    "We first organise the dataframe into the vector of targets $\\mathbf{y}$ and the design matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CO(GT)\n",
      "3453     0.7\n",
      "8244     0.5\n",
      "9075     3.9\n",
      "2499     1.8\n",
      "1180     1.4\n",
      "6900     1.2\n",
      "1063     2.6\n",
      "8121     2.5\n",
      "2539     1.1\n",
      "8686     1.4\n",
      "676      1.3\n",
      "6773     1.4\n",
      "1612     1.4\n",
      "3003     2.6\n",
      "4811     0.9\n",
      "8531     0.2\n",
      "781      0.5\n",
      "2626     0.4\n",
      "8746     1.0\n",
      "3814     1.0\n",
      "5141     1.5\n",
      "3678     1.5\n",
      "3020     2.0\n",
      "7958     3.1\n",
      "7580     3.0\n",
      "4021     1.9\n",
      "8225     1.3\n",
      "6688     5.6\n",
      "5445     3.1\n",
      "5568     5.9\n",
      "...      ...\n",
      "767      1.2\n",
      "4553     1.3\n",
      "172      2.8\n",
      "2205     2.3\n",
      "7140     1.0\n",
      "4744     3.4\n",
      "5875     1.9\n",
      "7852     2.1\n",
      "8633     1.1\n",
      "5633     2.4\n",
      "8855     3.4\n",
      "2401     3.5\n",
      "4388     2.5\n",
      "6633     2.0\n",
      "8553     0.5\n",
      "5444     2.3\n",
      "692      2.8\n",
      "8193     0.4\n",
      "2344     2.4\n",
      "6910     4.3\n",
      "2356     2.5\n",
      "5908     1.2\n",
      "8394     1.2\n",
      "1353     0.4\n",
      "7393     5.7\n",
      "8538     0.7\n",
      "3455     0.9\n",
      "784      1.0\n",
      "9248     0.7\n",
      "1741     1.2\n",
      "\n",
      "[5356 rows x 1 columns]\n",
      "      PT08.S1(CO)  C6H6(GT)  PT08.S2(NMHC)       NOx(GT)  PT08.S3(NOx)  \\\n",
      "3453    -1.133049 -0.868285      -0.924239 -9.219453e-01      0.934398   \n",
      "8244    -1.017636 -1.257734      -1.693758 -9.391065e-01      1.120946   \n",
      "9075     1.556422  1.158015       1.207035  1.401974e+00     -1.431511   \n",
      "2499    -0.711034 -0.144590       0.003402 -7.264977e-01      0.119112   \n",
      "1180     0.160975 -0.235441      -0.097220 -8.456730e-01      0.072721   \n",
      "6900    -1.105070 -1.057385      -1.248146 -6.263904e-01      1.064685   \n",
      "1063     1.106428  0.419722       0.569761 -5.167491e-01     -0.472119   \n",
      "8121     0.345169 -0.600801      -0.540916 -8.129201e-16     -0.558977   \n",
      "2539    -0.636423 -0.627459      -0.576373 -9.791494e-01      0.441870   \n",
      "8686    -0.953517 -0.752289      -0.749827  2.707616e-01      0.235581   \n",
      "676     -0.424250 -0.819759      -0.849491 -8.361390e-01      1.287754   \n",
      "6773    -0.235393 -0.623162      -0.570623 -2.688644e-01     -0.030917   \n",
      "1612    -0.361298 -0.606597      -0.548582 -7.837019e-01      0.804110   \n",
      "3003    -0.382282  0.450712       0.598510 -2.545633e-01     -0.355649   \n",
      "4811    -1.015304 -0.638157      -0.590748 -3.689717e-01      0.244464   \n",
      "8531    -1.774231 -1.366847      -2.072289 -1.068769e+00      3.554959   \n",
      "781     -1.148204 -1.275509      -1.743590 -1.036354e+00      3.104874   \n",
      "2626    -1.275275 -1.037883      -1.211730 -1.107859e+00      0.996580   \n",
      "8746    -0.576968 -0.948565      -1.054568 -4.371400e-01      0.043111   \n",
      "3814    -1.120225 -0.729553      -0.717244 -9.314793e-01      0.741927   \n",
      "5141    -0.177103 -0.508682      -0.422086 -3.737387e-01     -0.230296   \n",
      "3678     0.032738 -0.210639      -0.069429 -9.124112e-01     -0.510613   \n",
      "3020    -0.679558  0.560925       0.699132 -2.688644e-01     -0.105931   \n",
      "7958     0.705397  0.325071       0.480639  2.152779e+00     -0.840280   \n",
      "7580    -0.082675  0.547133       0.686674  8.666383e-01     -0.792903   \n",
      "4021     0.288046  0.216885       0.376183 -4.595449e-01     -0.774149   \n",
      "8225    -0.726189 -0.730226      -0.718203 -2.445526e-01      0.089501   \n",
      "6688     1.911987  2.776016       2.373294  3.163386e+00     -1.526265   \n",
      "5445     0.992181  0.741771       0.859170  5.176929e-01     -1.038673   \n",
      "5568     1.641524  2.431953       2.144258  1.876292e+00     -1.416705   \n",
      "...           ...       ...            ...           ...           ...   \n",
      "767     -0.820618 -0.951978      -1.060318 -9.124112e-01      1.864179   \n",
      "4553    -0.603781 -0.471884      -0.376087 -1.878251e-01      0.240516   \n",
      "172      1.550593  0.643466       0.772922 -2.593303e-01     -0.225361   \n",
      "2205     0.142322  0.467331       0.613843 -5.548852e-01     -0.177984   \n",
      "7140    -0.936031 -1.076430      -1.284562 -5.148423e-01      0.651120   \n",
      "4744     1.055133  1.358754       1.366114  1.347153e+00     -1.118622   \n",
      "5875    -0.538497 -0.337336      -0.214134  6.416353e-01     -0.073359   \n",
      "7852    -0.010396 -0.431309      -0.326255  4.924277e-01     -0.355649   \n",
      "8633    -0.622434 -0.700324      -0.676037 -4.519177e-01      0.170437   \n",
      "5633     0.599311  0.089881       0.249687  2.221380e-01     -0.785994   \n",
      "8855     0.992181  0.550312       0.689549  1.124534e+00     -1.131454   \n",
      "2401     0.487395  1.424201       1.416904 -1.830581e-01     -0.606354   \n",
      "4388     0.226259  0.798347       0.908043  6.654703e-01      0.182282   \n",
      "6633    -0.641087 -0.526844      -0.445085 -8.129201e-16      0.022383   \n",
      "8553    -1.508432 -1.284994      -1.771381 -8.129201e-16      1.845425   \n",
      "5444     0.466411  0.298994       0.455723 -1.258540e-01     -0.858047   \n",
      "692      0.152814  0.488194       0.633009 -3.594376e-01      0.071734   \n",
      "8193    -1.352217 -1.253849      -1.683217 -8.129201e-16      1.613473   \n",
      "2344     0.067712  0.262178       0.420265 -4.214088e-01     -0.244115   \n",
      "6910     0.903581  1.037884       1.109288  1.575970e+00     -1.031764   \n",
      "2356    -0.207414  0.456937       0.604260 -5.405842e-01     -0.322090   \n",
      "5908    -0.968673 -0.749631      -0.745994 -3.022335e-01      0.406337   \n",
      "8394    -0.554818 -0.901419      -0.976945  4.480513e-02      0.290855   \n",
      "1353    -1.719440 -1.263998      -1.711008 -8.129201e-16      3.189759   \n",
      "7393     1.891003  2.454593       2.159591  2.250026e+00     -1.601280   \n",
      "8538    -1.532914 -1.139887      -1.412975 -3.923300e-01      1.397314   \n",
      "3455    -0.927870 -0.688623      -0.659746 -9.314793e-01      0.648159   \n",
      "784     -0.330987 -1.009392      -1.159982 -8.838092e-01      1.511810   \n",
      "9248    -0.877741 -1.070466      -1.273062 -8.251749e-01      0.187217   \n",
      "1741     0.195948 -0.154244      -0.007139 -7.121967e-01     -0.126658   \n",
      "\n",
      "           NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)         T        RH        AH  \n",
      "3453 -6.358608e-01     -0.353309    -1.571916  2.601291 -1.841417  0.789272  \n",
      "8244 -1.353556e+00     -1.900713    -1.391688 -1.256243 -0.407585 -1.450951  \n",
      "9075  1.698784e+00      0.157145     2.000728  0.126209 -0.694351 -0.477967  \n",
      "2499 -4.649810e-01      0.565507    -0.239605  0.871052 -0.749949  0.415796  \n",
      "1180 -4.009011e-01      0.375910     0.303582  0.213156  0.035732  0.337193  \n",
      "6900 -7.853806e-01     -1.746848    -0.534353 -1.731552  0.350298 -1.462893  \n",
      "1063 -3.154612e-01      0.925013     0.995081 -0.186799  0.483439  0.140497  \n",
      "8121 -7.588586e-15     -1.153263     1.058286 -1.482305  0.998448 -1.045409  \n",
      "2539 -1.276660e+00      0.379556    -1.187054  1.743417 -1.187414  1.078294  \n",
      "8686  1.124201e+00     -1.829250    -0.907325 -0.934541 -1.308851 -1.706134  \n",
      "676  -6.358608e-01     -0.160795    -1.036238 -0.983810  1.523699 -0.282452  \n",
      "6773 -1.873014e-01     -0.624579    -0.456755 -0.589652  1.471027  0.240625  \n",
      "1612 -3.795412e-01     -0.086415    -0.737110  0.349372 -0.757264 -0.287859  \n",
      "3003  2.826180e-01      0.640617     0.376173  0.963795 -1.207897 -0.106599  \n",
      "4811 -9.349004e-01     -0.523947    -0.342861 -0.427352  0.732165 -0.014525  \n",
      "8531 -1.812795e+00     -2.582533    -2.051898 -2.085135 -0.634365 -1.930626  \n",
      "781  -1.319380e+00     -0.761672    -1.695823 -0.494011  0.053290 -0.561126  \n",
      "2626 -1.618420e+00     -0.022973    -1.027477  0.726141 -0.154470  0.970908  \n",
      "8746 -4.414851e-01     -1.202850    -0.190168 -1.586641  1.852895 -0.825911  \n",
      "3814 -1.169860e+00     -0.291326    -1.312212  2.549123 -1.981874  0.271411  \n",
      "5141 -1.105780e+00      0.299342    -0.381660  0.343576  1.124274  1.688284  \n",
      "3678 -1.169860e+00      0.879801    -0.044984  1.210144  0.342982  2.759060  \n",
      "3020  2.612580e-01      0.485293    -0.192671  2.079611 -2.079901 -0.527880  \n",
      "7958  3.584870e+00     -0.722294     0.897458 -1.821397  0.843360 -1.364099  \n",
      "7580  8.337052e-01     -0.743442     0.588943 -0.340405 -1.389321 -1.442089  \n",
      "4021 -5.504209e-01      1.076690     0.326110  0.830477  0.218619  1.668630  \n",
      "8225 -3.560452e-01     -1.629443    -0.400434 -1.001200 -0.944540 -1.552535  \n",
      "6688  1.649656e+00      1.156905     2.634655 -0.975116  0.878475 -0.602430  \n",
      "5445  9.037822e-02      0.883447     0.775429  1.071029 -0.293464  1.428426  \n",
      "5568  4.534977e-01      1.884666     1.565178  0.291407  1.023321  1.470702  \n",
      "...            ...           ...          ...       ...       ...       ...  \n",
      "767  -1.148500e+00     -0.767506    -1.576922  0.213155 -1.278126 -0.955571  \n",
      "4553 -7.640207e-01      0.171000    -0.429220  0.111718  1.399336  1.458481  \n",
      "172   4.534977e-01      0.818547     1.707232 -0.067972 -0.296390 -0.343214  \n",
      "2205  3.894178e-01      1.000122     0.006331  2.630273 -1.872142  0.734275  \n",
      "7140 -7.063487e-01     -1.493079    -0.458633 -1.780821  1.330570 -1.179052  \n",
      "4744  4.962177e-01      1.409215     1.191580  0.856561  0.018175  1.449432  \n",
      "5875  1.286537e+00     -0.608536    -0.374776 -0.960625  1.219375 -0.410128  \n",
      "7852  1.991416e+00     -1.417241     1.206599 -1.433035 -0.335893 -1.528030  \n",
      "8633  4.338628e-02     -1.431825    -1.147003 -0.546179 -1.108407 -1.375176  \n",
      "5633 -8.067406e-01      0.574987     0.247886  0.604416  0.900421  2.031668  \n",
      "8855  1.335665e+00     -0.015681     0.687818  0.134903 -0.865533 -0.630058  \n",
      "2401  1.393337e+00      1.741739     0.307962  1.476780 -1.155226  0.698588  \n",
      "4388  1.137017e+00      0.221316     0.292943  2.007155 -1.851659 -0.042973  \n",
      "6633 -7.588586e-15     -1.199204    -0.051868 -1.293920  0.521480 -1.077096  \n",
      "8553 -7.588586e-15     -2.320744    -1.563781 -2.186573 -0.458793 -1.922524  \n",
      "5444 -2.941013e-01      0.472897     0.563285  0.972490 -0.120819  1.488057  \n",
      "692   2.826180e-01      0.305905    -0.087538  0.337779 -1.557577 -1.142343  \n",
      "8193 -7.588586e-15     -1.994053    -1.744634 -1.227261 -0.692888 -1.559482  \n",
      "2344  4.938334e-03      1.262641     0.042001  1.085520 -0.608029  0.972145  \n",
      "6910  1.329257e+00     -0.352580     1.490083 -0.740360 -0.913815 -1.379506  \n",
      "2356 -1.659414e-01      1.148883     0.200326  0.644991 -0.338820  0.600551  \n",
      "5908 -1.642164e-02     -0.800321    -0.598810 -0.627329  1.418356  0.150397  \n",
      "8394  1.147697e+00     -1.241499    -0.722091 -1.606928  1.775351 -0.873793  \n",
      "1353 -7.588586e-15     -0.450295    -1.727112 -0.583856  1.314476  0.149490  \n",
      "7393  1.429648e+00      1.329730     1.741025 -0.418657  0.502459 -0.163304  \n",
      "8538 -4.137171e-01     -2.329494    -1.668914 -1.670689 -1.172783 -1.945706  \n",
      "3455 -6.572208e-01     -0.196527    -1.461776  2.407110 -1.719980  0.858478  \n",
      "784  -7.853806e-01     -0.351851    -1.226479 -0.554873  0.607802 -0.266101  \n",
      "9248 -1.090828e+00     -0.808342    -0.636983 -0.438945  1.141831  0.253132  \n",
      "1741 -7.426607e-01      0.573529     0.078297 -0.065074  0.217156  0.095852  \n",
      "\n",
      "[5356 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here to get y and X\n",
    "y = splitData_train_new.loc[:,[\"CO(GT)\"]]\n",
    "X = splitData_train_new.drop([\"CO(GT)\"], axis = 1)\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: training with closed form expression for $\\mathbf{w}$ (3 marks)\n",
    "\n",
    "To find the optimal value of $\\mathbf{w}$ using the closed form expression that you derived before, we need to know the value of the regularisation parameter $\\alpha$ in advance. We will determine the value by using part of the training data for finding the parameters $\\mathbf{w}$ and another part of the training data to choose the best $\\alpha$ from a set of predefined values.\n",
    "\n",
    "* Use `np.logspace(start, stop, num)` to create a set of values for $\\alpha$ in log scale. Use the following parameters `start=-3`, `stop=2` and `num=20`. \n",
    "\n",
    "* Randomly split the training data into what is properly called the training set and the validation set. As before, make sure that you use a random seed that corresponds to the last five digits of your student UCard. Use 70% of the data for the training set and 30% of the data for the validation set.\n",
    "\n",
    "* For each value that you have for $\\alpha$ from the previous step, use the training set to compute $\\mathbf{w}$ and then measure the mean-squared error (MSE) over the validation data. After this, you will have `num=20` MSE values. Choose the value of $\\alpha$ that leads to the lower MSE and save it. You will use it at the test stage.\n",
    "\n",
    "* What was the best value of $\\alpha$? Is there any explanation for that?\n",
    "\n",
    "#### Question 6 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-03 1.83298071e-03 3.35981829e-03 6.15848211e-03\n",
      " 1.12883789e-02 2.06913808e-02 3.79269019e-02 6.95192796e-02\n",
      " 1.27427499e-01 2.33572147e-01 4.28133240e-01 7.84759970e-01\n",
      " 1.43844989e+00 2.63665090e+00 4.83293024e+00 8.85866790e+00\n",
      " 1.62377674e+01 2.97635144e+01 5.45559478e+01 1.00000000e+02]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "#  create a set of values for  in log scale.\n",
    " = np.logspace(-3, 2, 20)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly split the training data into what is properly called the training set and the validation set\n",
    "def splitTrainData(data,seed,rate):\n",
    "    np.random.seed(seed)\n",
    "    data_index = np.random.permutation(data.index)\n",
    "    data = data.reindex(data_index)\n",
    "    num = int(len(data_index)*rate)\n",
    "    data_train = data.iloc[num:,:]\n",
    "    data_test = data.iloc[:num,:]\n",
    "    return (data_train,data_test)\n",
    "       \n",
    "splitTrainData_train,splitTrainData_val = splitTrainData(splitData_train_new,18979,0.3)\n",
    "#splitTrainData_train is 70% of the training data for the training set.\n",
    "#splitTrainData_val is 30% of the training data for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get y and X for training set and validation set\n",
    "y_train = splitTrainData_train.loc[:,[\"CO(GT)\"]]\n",
    "X_train = splitTrainData_train.drop([\"CO(GT)\"], axis = 1)\n",
    "y_val = splitTrainData_val.loc[:,[\"CO(GT)\"]]\n",
    "X_val = splitTrainData_val.drop([\"CO(GT)\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34449939]\n",
      " [ 0.58217969]\n",
      " [ 0.02238273]\n",
      " [ 0.48682422]\n",
      " [ 0.00496068]\n",
      " [ 0.16563739]\n",
      " [ 0.39261027]\n",
      " [-0.30960121]\n",
      " [-0.15142552]\n",
      " [-0.10038665]\n",
      " [-0.06494573]]\n"
     ]
    }
   ],
   "source": [
    "#use the training set to compute ,we assume  = [0].\n",
    "from scipy.sparse import identity\n",
    "n = len(y_train)\n",
    "length = len(np.dot(X_train.T, X_train))\n",
    "I = identity(length).toarray()\n",
    "w = np.linalg.solve(np.dot(X_train.T, X_train)*(2/n) + [0]*I, np.dot(X_train.T, y_train)*(2/n))#when  equals [0]\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO(GT)    4.931072\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#measure the mean-squared error (MSE) over the validation data,when  equals [0].\n",
    "n = len(y_val)\n",
    "f = np.dot(X_val, w)\n",
    "MSE_init = ((y_val - f)**2 / n).sum()\n",
    "print(MSE_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12742749857031335\n"
     ]
    }
   ],
   "source": [
    "#Choose the value of  that leads to the lower MSE and save it. \n",
    "choose_ = 0\n",
    "n = len(y_val)\n",
    "for i in range(20):\n",
    "    w = np.linalg.solve(np.dot(X_train.T, X_train)*(2/n) + [i]*I, np.dot(X_train.T, y_train)*(2/n))\n",
    "    f = np.dot(X_val, w)\n",
    "    MSE = ((y_val - f)**2 / n ).sum()\n",
    "    if MSE.iloc[0] < MSE_init.iloc[0]:\n",
    "        MSE_init = MSE\n",
    "        choose_ = [i]\n",
    "print(choose_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer to the last question here.\n",
    "\n",
    "Since the MSE obtained is the smallest, the best value of  is 0.12742749857031335\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: validation with the closed form expression for $\\mathbf{w}$ (2 marks)\n",
    "\n",
    "We are going to deal now with the test data to perform the validation of the model. Remember that the test data might also contain missing values in the target variable and in the input features.\n",
    "\n",
    "* Remove the rows of the test data for which the labels have missing values. \n",
    "* If you remove any feature at the training stage, you also need to remove the same features from the test stage.\n",
    "* Replace the missing values on each feature variables with the mean value you computed in the training data.\n",
    "* Normalise the test data using the means and standard deviations computed from the training data\n",
    "* Compute again $\\mathbf{w}$ for the value of $\\alpha$ that best performed on the validation set using ALL the training data (not all the training set).\n",
    "* Report the MSE on the preprocessed test data and an histogram with the absolute error.\n",
    "* Does the regularisation have any effect on the model? Explain your answer.\n",
    "\n",
    "#### Question 7 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO(GT)    4.95737\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "#Remove the rows of the test data for which the labels have missing values. \n",
    "splitData_test_new = splitData_test[splitData_test['CO(GT)'] != -200]\n",
    "#Remove the same features from the test stage\n",
    "del splitData_test_new['NMHC(GT)']\n",
    "#Replace the missing values on each feature variables with the mean value you computed in the training data\n",
    "splitData_test_new = splitData_test_new.replace(to_replace = -200, value = np.NaN,inplace = False)\n",
    "for i in range(12):\n",
    "    splitData_test_new[splitData_test_new.columns[i]].fillna(mean[i], inplace=True)\n",
    "#Normalise the test data using the means and standard deviations computed from the training data\n",
    "for i in range(1,12):\n",
    "    splitData_test_new[splitData_test_new.columns[i]] = (splitData_test_new[splitData_test_new.columns[i]] - mean[i])/std[i]\n",
    "\n",
    "#Compute again w for the value of  that best performed on the validation set using ALL the training data \n",
    "n = len(y)\n",
    "length = len(np.dot(X.T, X))\n",
    "I = identity(length).toarray()\n",
    "w = np.linalg.solve(np.dot(X.T, X)*(2/n) + choose_ * I, np.dot(X.T, y)*(2/n))#when  equals [0]\n",
    "#Report the MSE on the preprocessed test data \n",
    "#Divide the test data to X and y\n",
    "y_test = splitData_test_new.loc[:,[\"CO(GT)\"]]\n",
    "X_test = splitData_test_new.drop([\"CO(GT)\"], axis = 1)\n",
    "f_test = np.dot(X_test, w)\n",
    "n = len(y_test)\n",
    "MSE_test = ((y_test - f_test)**2 / n).sum()\n",
    "print(MSE_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0000021D8209DA58>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#histogram with the absolute error\n",
    "abs((y_test - np.dot(X_test, w))).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the explanation to your answer here.\n",
    "\n",
    "Adding regularisation have effect on the model. Specifically, it gets a smoother decision boundary and improves model training results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: training with gradient descent and validation (5 marks)\n",
    "\n",
    "\n",
    "Use gradient descent to iteratively compute the value of $\\mathbf{w}_{\\text{new}}$. Instead of using all the training set to compute the gradient, use a subset of $B$ datapoints in the training set. This is sometimes called minibatch gradient descent where $B$ is the size of the minibacth. When using gradient descent with minibatches, you need to find the best values for three parameters: $\\eta$, the learning rate, $B$, the number of datapoints in the minibatch and $\\alpha$, the regularisation parameter.\n",
    "\n",
    "* As you did on Question 6, create a grid of values for the parameters $\\alpha$ and $\\eta$ using `np.logspace` and a grid of values for $B$ using np.linspace. Because you need to find \n",
    " three parameters, start with `num=5` and see if you can increase it.\n",
    "\n",
    "* Use the same training set and validation set that you used in Question 6.\n",
    "\n",
    "* For each value that you have of $\\alpha$, $\\eta$ and $B$ from the previous step, use the training set to compute $\\mathbf{w}$ using minibatch gradient descent and then measure the MSE over the validation data. For the minibatch gradient descent choose to stop the iterative procedure after $500$ iterations.\n",
    "\n",
    "* Choose the values of $\\alpha$, $\\eta$ and $B$ that lead to the lower MSE and save them. You will use them at the test stage.\n",
    "\n",
    "*3 marks of out of the 5 marks*\n",
    "\n",
    "\n",
    "* Use the test set from Question 7 and provide the MSE obtained by having used minibatch training with the best values for $\\alpha$, $\\eta$ and $B$ over the WHOLE training data (not only the training set).\n",
    "\n",
    "* Compare the performance of the closed form solution and the minibatch solution. Are the performances similar? Are the parameters $\\mathbf{w}$ and $\\alpha$ similar in both approaches? Please comment on both questions.\n",
    "\n",
    "*2 marks of out of the 5 marks*\n",
    "\n",
    "#### Question 8 Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20275742]\n",
      " [-0.09506076]\n",
      " [-1.39881904]\n",
      " [-1.19918006]\n",
      " [ 0.06346507]\n",
      " [-0.78156959]\n",
      " [ 0.22204161]\n",
      " [ 0.83940976]\n",
      " [ 0.22453323]\n",
      " [-0.34610923]\n",
      " [-1.78726769]]\n",
      "CO(GT)    22.827734\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Write the code for your answer here\n",
    "#create a grid of value for the parameters ,  and B\n",
    " = np.logspace(-3, 2, 5)\n",
    " = np.logspace(-5, -2, 5)\n",
    "B = np.linspace(50,500,5)\n",
    "#Convert B[i] type to int\n",
    "B = B.astype(np.int16)\n",
    "for i in range(5):\n",
    "    B[i] = np.asscalar(B[i])\n",
    "#use the training set to compute w using minibatch gradient descent \n",
    "#(Assume  = [0],  = [0], and B = B[0])\n",
    "n = B[0]\n",
    "length = len(w)\n",
    "I = identity(length).toarray()\n",
    "w = np.random.normal(size=(X_train.shape[1],1))# Initialize w\n",
    "\n",
    "def MGD_one(max_iter):\n",
    "    for iteration in range(max_iter):\n",
    "        global w\n",
    "        np.random.shuffle(splitTrainData_train.index.values)\n",
    "        y_train = splitTrainData_train.loc[:,[\"CO(GT)\"]]\n",
    "        X_train = splitTrainData_train.drop([\"CO(GT)\"], axis = 1)\n",
    "        y_batch = y_train.iloc[0:B[0],:]\n",
    "        X_batch = X_train.iloc[0:B[0],:]\n",
    "        obj = [0]*(-2/n*np.dot(X_batch.T, y_batch) + 2/n*np.dot(X_batch.T, np.dot(X_batch,w)) + [0]*np.dot(I,w) )\n",
    "        w = w - obj\n",
    "    print(w)\n",
    "\n",
    "max_iter = 500\n",
    "MGD_one(max_iter)\n",
    "\n",
    "#measure the MSE over the validation data\n",
    "n = len(y_val)\n",
    "f = np.dot(X_val, w)\n",
    "MSE_init = ((y_val - f)**2 / n).sum()\n",
    "print(MSE_init)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31622776601683794\n",
      "0.01\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "#Choose the values of ,  and B that lead to the lower MSE\n",
    "choose_ = 0\n",
    "choose_ = 0\n",
    "choose_B = 0\n",
    "for i in range(len()):\n",
    "    for j in range(len()):\n",
    "        for k in range(len(B)):\n",
    "            n = B[k]\n",
    "            w = np.random.normal(size=(X_train.shape[1],1))\n",
    "            length = len(w)\n",
    "            I = identity(length).toarray()\n",
    "            def MGD_two(max_iter):\n",
    "                for iteration in range(max_iter):\n",
    "                    global w\n",
    "                    np.random.shuffle(splitTrainData_train.values)\n",
    "                    y_train = splitTrainData_train.loc[:,[\"CO(GT)\"]]\n",
    "                    X_train = splitTrainData_train.drop([\"CO(GT)\"], axis = 1)\n",
    "                    y_batch = y_train.iloc[0:B[k],:]\n",
    "                    X_batch = X_train.iloc[0:B[k],:]\n",
    "                    obj = [j]*(-2/n*np.dot(X_batch.T, y_batch) + 2/n*np.dot(X_batch.T, np.dot(X_batch,w)) + [i]*np.dot(I,w) )\n",
    "                    w = w - obj\n",
    "            max_iter = 500\n",
    "            MGD_two(max_iter)\n",
    "            n = len(y_val)\n",
    "            f = np.dot(X_val, w)\n",
    "            MSE = ((y_val - f)**2 / n).sum()\n",
    "            if MSE.iloc[0] < MSE_init.iloc[0]:\n",
    "                MSE_init = MSE\n",
    "                choose_ = [i]\n",
    "                choose_ = [j]\n",
    "                choose_B = B[k]\n",
    "        \n",
    "print(choose_)\n",
    "print(choose_)\n",
    "print(choose_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16230327]\n",
      " [ 0.24508081]\n",
      " [ 0.24930375]\n",
      " [ 0.40707725]\n",
      " [ 0.05741468]\n",
      " [ 0.11515122]\n",
      " [ 0.3416381 ]\n",
      " [ 0.09034843]\n",
      " [-0.06325806]\n",
      " [-0.0387681 ]\n",
      " [-0.13926793]]\n",
      "CO(GT)    4.959738\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#provide the MSE obtained by having used minibatch training with the best values for  ,  and B over the WHOLE training data\n",
    "\n",
    "n =  choose_B\n",
    "w = np.random.normal(size=(X_train.shape[1],1))\n",
    "length = len(w)\n",
    "I = identity(length).toarray()\n",
    "def MGD_three(max_iter):\n",
    "    for iteration in range(max_iter):\n",
    "        global w\n",
    "        np.random.shuffle(splitData_train_new.values)\n",
    "        y_train = splitData_train_new.loc[:,[\"CO(GT)\"]]\n",
    "        X_train = splitData_train_new.drop([\"CO(GT)\"], axis = 1)\n",
    "        y_batch = y_train.iloc[0:choose_B,:]\n",
    "        X_batch = X_train.iloc[0:choose_B,:]\n",
    "        obj = choose_ *(-2/n*np.dot(X_batch.T, y_batch) + 2/n*np.dot(X_batch.T, np.dot(X_batch,w)) + choose_ *np.dot(I,w) )\n",
    "        w = w - obj\n",
    "    print(w)\n",
    "\n",
    "max_iter = 500\n",
    "MGD_three(max_iter)\n",
    "\n",
    "n = len(y_test)\n",
    "f_test = np.dot(X_test, w)\n",
    "MSE_test = ((y_test - f_test)**2 / n).sum()\n",
    "print(MSE_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the answer to your last question here.\n",
    "\n",
    "The performance of the closed form solution and the minibatch solution is very similar, we can see that the MSE on the closed form and the minibatch solution are 4.95737 and 4.959738 separately.\n",
    "\n",
    "The parameters  and w are not similar in these two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
